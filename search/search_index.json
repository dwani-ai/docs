{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"dwani.ai - Discovery","text":"<p>dwani.ai provides secure AI document analytics tailored for proprietary data.</p> <p>Try it now at app.dwani.ai</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>dwani.ai empowers users with the following features:</p> <ul> <li>Text and Image Inference: Process and analyze text and images with state-of-the-art models.</li> <li>Document Processing: Extract, translate, and query documents efficiently.</li> <li>Speech Processing: Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) for Indian + European languages.</li> <li>Translation: Seamless translation across major Indian + European languages.</li> <li>Scalability: Load balancing and API orchestration for robust performance.</li> </ul>"},{"location":"index.html#system-architecture","title":"System Architecture","text":"<p>The dwani.ai engine is designed for modularity and scalability. Below is a high-level overview of the inference pipeline:</p> <p></p>"},{"location":"index.html#component-setup-discovery","title":"Component Setup - Discovery","text":"<p>Each component is modular and can be set up independently. Refer to the linked guides for detailed setup instructions.</p> Component Description Setup Guide vLLM Server Text and image inference vLLM Deployment API Server API gateway &amp; Swagger setup API Server Setup Discovery Server Discovery Server Discovery Setup Proxy Server Load balancer Proxy Server Setup"},{"location":"index.html#component-setup-multimodal-inference","title":"Component Setup - Multimodal Inference","text":"<p>For multimodal inference, the following components are available. Each is independently configurable.</p> Component Description Setup Guide Docs API Server Document extraction, translation, query Docs API Setup Translate Server Indian language translation Translate Server TTS Server Text-to-Speech (Indian languages) TTS Server ASR Server Automatic Speech Recognition ASR Server"},{"location":"index.html#model-dependencies","title":"Model Dependencies","text":"<p>The following models power Dwani.ai's functionality:</p> Task Models Used Text + Vision <code>google/gemma-3-27b-it</code>, <code>google/gemma-3-12b-it</code>, <code>google/gemma-3-4b-it</code> Text <code>Qwen/Qwen3-32B</code>, <code>Qwen/Qwen3-14B</code> Vision <code>vikhyatk/moondream2</code> Speech Synthesis (TTS) <code>ai4bharat/IndicF5</code>, <code>onnx-community/Kokoro-82M-v1.0-ONNX</code> Translation <code>ai4bharat/IndicTrans3-beta</code>, <code>ai4bharat/indictrans2-indic-indic-1B</code>, etc. Automatic Speech Recognition (ASR) <code>ai4bharat/indic-conformer-600m-multilingual</code>, <code>Systran/faster-whisper-large-v3</code> Text Analysis <code>ai4bharat/Cadence</code>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please open issues or submit pull requests to improve dwani.ai.</p> <p>Contribute on GitHub</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <p>Built with \u2764\ufe0f by the dwani.ai team</p>"},{"location":"node-sdk.html","title":"Node sdk","text":"<p>Node SDK</p>"},{"location":"python-sdk.html","title":"dwani.ai - python library","text":""},{"location":"python-sdk.html#install-the-library","title":"Install the library","text":"<pre><code>pip install --upgrade dwani\n</code></pre>"},{"location":"python-sdk.html#languages-supported","title":"Languages supported","text":"<ul> <li>Indian<ul> <li>Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi Odia, Punjabi, Tamil, Telugu</li> </ul> </li> <li>European<ul> <li>English, German</li> </ul> </li> </ul>"},{"location":"python-sdk.html#setup-the-credentials","title":"Setup the credentials","text":"<pre><code>import dwani\nimport os\n\ndwani.api_key = os.getenv(\"DWANI_API_KEY\")\n\ndwani.api_base = os.getenv(\"DWANI_API_BASE_URL\")\n</code></pre> <ul> <li>Source Code : https://github.com/dwani-ai/dwani-python-sdk</li> </ul>"},{"location":"python-sdk.html#document-ocr","title":"Document - OCR","text":"<p><pre><code>result = dwani.Documents.run_ocr_page(file_path=\"dwani-workshop.pdf\", page_number=1, model=\"gemma3\")\nprint(result)\n</code></pre> <pre><code>{'page_content': \"Here's the plain text extracted from the image:\\n\\ndwani's Goals\\n\\nTo integrate and enhance the following models and services for Kannada:\\n\\n*   **Automatic Speech Recognition (ASR):**\"}\n</code></pre></p>"},{"location":"python-sdk.html#document-summary","title":"Document - Summary","text":"<pre><code>result = dwani.Documents.summarize_all(\n            file_path=\"dwani-workshop.pdf\", model=\"gemma3\" , tgt_lang=\"english\"  \n    )\n\nprint(\"Document Query Response: gemma3- \", result[\"summary\"])\n</code></pre>"},{"location":"python-sdk.html#text-query","title":"Text Query","text":"<ul> <li>gemma3 (default)</li> </ul> <p><pre><code>resp = dwani.Chat.create(prompt=\"Hello!\", src_lang=\"english\", tgt_lang=\"kannada\", model=\"gemma3\")\nprint(resp)\n</code></pre> <pre><code>{'response': '\u0ca8\u0cae\u0cb8\u0ccd\u0ca4\u0cc6! \u0cad\u0cbe\u0cb0\u0ca4 \u0cae\u0ca4\u0ccd\u0ca4\u0cc1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0cb5\u0ca8\u0ccd\u0ca8\u0cc1 \u0c97\u0cae\u0ca8\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf\u0c9f\u0ccd\u0c9f\u0cc1\u0c95\u0cca\u0c82\u0ca1\u0cc1 \u0c87\u0c82\u0ca6\u0cc1 \u0ca8\u0cbf\u0cae\u0ccd\u0cae \u0caa\u0ccd\u0cb0\u0cb6\u0ccd\u0ca8\u0cc6\u0c97\u0cb3\u0cbf\u0c97\u0cc6 \u0ca8\u0cbe\u0ca8\u0cc1 \u0ca8\u0cbf\u0cae\u0c97\u0cc6 \u0cb9\u0cc7\u0c97\u0cc6 \u0cb8\u0cb9\u0cbe\u0caf \u0cae\u0cbe\u0ca1\u0cb2\u0cbf?'}\n</code></pre></p>"},{"location":"python-sdk.html#vision-query","title":"Vision Query","text":"<ul> <li>gemma3 (default)     <pre><code>result = dwani.Vision.caption(\n    file_path=\"image.png\",\n    query=\"Describe this logo\",\n    src_lang=\"english\",\n    tgt_lang=\"kannada\",\n    model=\"gemma3\"\n)\nprint(result)\n</code></pre> <pre><code>{'answer': '\u0c92\u0c82\u0ca6\u0cc1 \u0cb5\u0cbe\u0c95\u0ccd\u0caf\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0c9a\u0cbf\u0ca4\u0ccd\u0cb0\u0ca6 \u0cb8\u0cbe\u0cb0\u0cbe\u0c82\u0cb6\u0cb5\u0ca8\u0ccd\u0ca8\u0cc1 \u0c87\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cc0\u0ca1\u0cb2\u0cbe\u0c97\u0cbf\u0ca6\u0cc6\u0c83 \u0caa\u0ccd\u0cb0\u0c95\u0c9f\u0ca3\u0cc6\u0caf \u0c85\u0cb5\u0cb2\u0ccb\u0c95\u0ca8\u0cb5\u0cc1 \u0caa\u0ccd\u0cb0\u0cb8\u0ccd\u0ca4\u0cc1\u0ca4 \u0c85\u0cb0\u0cb5\u0ca4\u0ccd\u0ca4\u0ca8\u0cbe\u0cb2\u0ccd\u0c95\u0cc1 \u0ca6\u0cc7\u0cb6\u0c97\u0cb3\u0cc1/\u0caa\u0ccd\u0cb0\u0ca6\u0cc7\u0cb6\u0c97\u0cb3\u0ca8\u0ccd\u0ca8\u0cc1 \u0cb8\u0cc7\u0cb0\u0cbf\u0cb8\u0cb2\u0cbe\u0c97\u0cbf\u0ca6\u0cc6 \u0cae\u0ca4\u0ccd\u0ca4\u0cc1 \u0c87\u0ca8\u0ccd\u0ca8\u0cc2 \u0cb9\u0ca6\u0cbf\u0ca8\u0cbe\u0cb0\u0cc1 \u0caa\u0ccd\u0cb0\u0ca6\u0cc7\u0cb6\u0c97\u0cb3\u0ca8\u0ccd\u0ca8\u0cc1 \u0cb8\u0cc7\u0cb0\u0cbf\u0cb8\u0cac\u0cc7\u0c95\u0cbe\u0c97\u0cbf\u0ca6\u0cc6. \u0c92\u0ca6\u0c97\u0cbf\u0cb8\u0cb2\u0cbe\u0ca6 \u0c9a\u0cbf\u0ca4\u0ccd\u0cb0\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0cb2\u0cbe\u0c82\u0c9b\u0ca8\u0cb5\u0cc1 \u0c95\u0cbe\u0ca3\u0cbf\u0cb8\u0cc1\u0cb5\u0cc1\u0ca6\u0cbf\u0cb2\u0ccd\u0cb2.'}\n</code></pre></li> </ul>"},{"location":"python-sdk.html#speech-to-text-automatic-speech-recognition-asr","title":"Speech to Text -  Automatic Speech Recognition (ASR)","text":"<p><pre><code>result = dwani.ASR.transcribe(file_path=\"kannada_sample.wav\", language=\"kannada\")\nprint(result)\n</code></pre> <pre><code>{'text': '\u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0caf\u0cbe\u0cb5\u0cc1\u0ca6\u0cc1'}\n</code></pre></p>"},{"location":"python-sdk.html#translate","title":"Translate","text":"<p><pre><code>resp = dwani.Translate.run_translate(sentences=\"hi, i am gaganyatri\", src_lang=\"english\", tgt_lang=\"kannada\")\nprint(resp)\n</code></pre> <pre><code>{'translations': ['\u0cb9\u0cbe\u0caf\u0ccd, \u0ca8\u0cbe\u0ca8\u0cc1 \u0c97\u0c97\u0ca8\u0caf\u0cbe\u0ca4\u0ccd\u0cb0\u0cbf']}\n</code></pre></p>"},{"location":"python-sdk.html#text-to-speech-speech-synthesis","title":"Text to Speech -  Speech Synthesis","text":"<pre><code>response = dwani.Audio.speech(input=\"\u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0caf\u0cbe\u0cb5\u0cc1\u0ca6\u0cc1\", response_format=\"wav\", language=\"kannada\")\nwith open(\"output.wav\", \"wb\") as f:\n    f.write(response)\n</code></pre>"},{"location":"python-sdk.html#document-extract-text","title":"Document - Extract Text","text":"<p><pre><code>result = dwani.Documents.run_extract(file_path = \"dwani-workshop.pdf\", page_number=1, src_lang=\"english\",tgt_lang=\"kannada\" )\nprint(result)\n</code></pre> <pre><code>{'pages': [{'processed_page': 1, 'page_content': ' a plain text representation of the document', 'translated_content': '\u0ca1\u0cbe\u0c95\u0ccd\u0caf\u0cc1\u0cae\u0cc6\u0c82\u0c9f\u0ccd\u0ca8 \u0cb8\u0cb0\u0cb3 \u0caa\u0ca0\u0ccd\u0caf \u0caa\u0ccd\u0cb0\u0cbe\u0ca4\u0cbf\u0ca8\u0cbf\u0ca7\u0ccd\u0caf\u0cb5\u0ca8\u0ccd\u0ca8\u0cc1 \u0c87\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cc0\u0ca1\u0cb2\u0cbe\u0c97\u0cbf\u0ca6\u0cc6, \u0c85\u0ca6\u0ca8\u0ccd\u0ca8\u0cc1 \u0cb8\u0ccd\u0cb5\u0cbe\u0cad\u0cbe\u0cb5\u0cbf\u0c95\u0cb5\u0cbe\u0c97\u0cbf \u0c93\u0ca6\u0cc1\u0cb5\u0c82\u0ca4\u0cc6\u0c83'}]}\n</code></pre></p>"},{"location":"arm64/index.html","title":"readme","text":"<p>Setup vllm for arm64</p> <ul> <li>Install - python3.12 - python_3_12.md</li> <li>Create vllm wheel - vllm-setup.md</li> <li> <p>Create vllm-docker - vllm_docker_build.md</p> </li> <li> <p>Open Items</p> <ul> <li>flash-infer</li> <li>xformers</li> </ul> </li> </ul>"},{"location":"arm64/flashinfer.html","title":"flashinfer","text":"<p>Flash Infer for arm64</p> <ul> <li>https://arxiv.org/abs/2501.01005</li> </ul> <p>git clone https://github.com/flashinfer-ai/flashinfer.git --recursive</p> <p>pip install ninja</p> <p>export TORCH_CUDA_ARCH_LIST=\"90\"</p> <p>cd flashinfer pip install --no-build-isolation --verbose .</p> <p>python -m flashinfer.aot  # Compile AOT kernels python -m pip install --no-build-isolation --verbose .</p>"},{"location":"arm64/python_3_12.html","title":"Python 3 12","text":"<p>python 3_12</p> <pre><code>sudo apt update\nsudo apt upgrade -y\n\nsudo apt install python3.12 python3.12-venv python3.12-dev -y\n\npython3.12 -m venv venv\n\nsource venv/bin/activate\n</code></pre>"},{"location":"arm64/torch_2_8.html","title":"torch","text":"<p>torch 2.8 for arm64</p> <p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | INSTALLER_DOWNLOAD_URL=https://wheelnext.astral.sh sh\n</code></pre> <pre><code>uv venv venv --python 3.12\nsource venv/bin/activate\n\nuv pip install torch torchvision\n</code></pre></p>"},{"location":"arm64/vllm-setup.html","title":"vLLM wheel","text":"<p>vLLM for arm64 </p> <ul> <li>Create vllm library wheel <pre><code>sudo apt-get install build-essential libnuma-dev -y\n\nsudo apt remove cmake -y\n\nwget https://github.com/Kitware/CMake/releases/download/v3.31.8/cmake-3.31.8-linux-aarch64.sh\nchmod +x cmake-3.31.8-linux-aarch64.sh\nsudo ./cmake-3.31.8-linux-aarch64.sh --prefix=/usr/local --exclude-subdir\n\npip uninstall torch torchvision torchaudio\n\npip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128\n\n\n\ngit clone https://github.com/vllm-project/vllm.git\n\ncd vllm\n\n\npython use_existing_torch.py \n\npip install --upgrade setuptools twine setuptools-scm\n\n\npip install -r requirements/cuda.txt\n\nexport MAX_JOBS=16\nexport NVCC_THREADS=4\nexport TORCH_CUDA_ARCH_LIST=\"\"\nexport VLLM_TARGET_DEVICE=cuda\n\npython setup.py bdist_wheel\npip install dist/*.whl\n</code></pre></li> </ul>"},{"location":"arm64/vllm_docker_build.html","title":"vLLM docker build","text":"<p>Vllm - Docker Build</p> <ul> <li>Docker Images </li> <li>https://hub.docker.com/r/dwani/vllm-arm64 </li> <li> <p>docker pull dwani/vllm-arm64:latest</p> </li> <li> <p>Create vLLM arm64 - Docker Image ```bash</p> </li> </ul> <p>git clone https://github.com/vllm-project/vllm.git</p> <p>cd vllm</p> <p>DOCKER_BUILDKIT=1 sudo docker build . \\ --file docker/Dockerfile \\ --target vllm-openai \\ --platform \"linux/arm64\" \\ -t vllm/vllm-openai:latest \\ --build-arg max_jobs=16 \\ --build-arg nvcc_threads=4 \\ --build-arg VLLM_MAX_SIZE_MB=1000 \\ --build-arg torch_cuda_arch_list=\"\"</p> <p>sudo docker tag vllm/vllm-openai:latest dwani/vllm-arm64:latest</p> <p>sudo docker push dwani/vllm-arm64:latest</p> <p>Test Build -    sudo docker run --gpus all \\   -p 8000:8000 \\   dwani/vllm-arm64:latest \\   --model Qwen/Qwen3-0.6B \\   --port 8000</p> <p>sudo docker run --runtime nvidia -it --rm -p 9000:9000 dwani/vllm-arm64:latest --model RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.7 --tensor-parallel-size 1 --max-model-len 65536 --dtype bfloat16 --disable-log-requests</p> <p>https://docs.vllm.ai/en/latest/deployment/docker.html#building-vllms-docker-image-from-source</p> <p>--  Extra - vllm  - python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000</p> <ul> <li>openweb-ui</li> </ul> <p>docker run -d \\     --name open-webui \\     -p 3000:8080 \\     -v open-webui:/app/backend/data \\     -e OPENAI_API_BASE_URL=http://0.0.0.0:8000/v1 \\     --restart always \\     ghcr.io/open-webui/open-webui:main</p> <p>sudo docker run --gpus all \\   -p 8000:8000 \\   vllm/vllm-openai \\   --model Qwen/Qwen3-0.6B \\   --port 8000</p> <p>--</p> <p>https://docs.vllm.ai/en/latest/deployment/docker.html#building-for-arm64aarch64</p>"},{"location":"arm64/xfomers.html","title":"xfomers","text":"<p>xformers</p> <p>https://github.com/facebookresearch/xformers</p> <p>pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu128</p> <ul> <li>build</li> </ul>"},{"location":"arm64/xfomers.html#optional-makes-the-build-much-faster","title":"(Optional) Makes the build much faster","text":"<p>pip install ninja</p>"},{"location":"arm64/xfomers.html#set-torch_cuda_arch_list-if-running-and-building-on-different-gpu-types","title":"Set TORCH_CUDA_ARCH_LIST if running and building on different GPU types","text":""},{"location":"arm64/xfomers.html#note-pytorch-must-already-be-installed","title":"NOTE: pytorch must already be installed!","text":"<p>pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers</p>"},{"location":"arm64/xfomers.html#this-can-take-dozens-of-minutes","title":"(this can take dozens of minutes)","text":""},{"location":"cuda/index.html","title":"readme","text":"<ul> <li>Install - cuda on Server<ul> <li>cuda_12_9-setup.md</li> </ul> </li> <li>Enable Docker on Server<ul> <li>cuda_docker.md</li> </ul> </li> </ul>"},{"location":"cuda/cuda_12_9-setup.html","title":"cuda driver","text":"<p>CUDA Setup</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local</p> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</p> <p>sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-</p> <p>wget https://developer.download.nvidia.com/compute/cuda/12.9.1/local_installers/cuda-repo-ubuntu2204-12-9-local_12.9.1-575.57.08-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu2204-12-9-local_12.9.1-575.57.08-1_amd64.deb</p> <p>sudo cp /var/cuda-repo-ubuntu2204-12-9-local/cuda-*-keyring.gpg /usr/share/keyrings/</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda-toolkit-12-9</p> <p>sudo apt-get install -y cuda-drivers</p> <p>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\   &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</p> <p>sudo apt-get update</p> <p>sudo apt-get install -y nvidia-container-toolkit</p> <p>for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done</p>"},{"location":"cuda/cuda_12_9-setup.html#add-dockers-official-gpg-key","title":"Add Docker's official GPG key:","text":"<p>sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc</p>"},{"location":"cuda/cuda_12_9-setup.html#add-the-repository-to-apt-sources","title":"Add the repository to Apt sources:","text":"<p>echo \\   \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\   $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\   sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update</p> <p>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</p> <p>sudo docker run hello-world</p> <p>sudo groupadd docker</p> <p>sudo usermod -aG docker $USER</p> <p>sudo nvidia-ctk runtime configure --runtime=docker</p> <p>sudo systemctl restart docker</p> <p>nvidia-ctk runtime configure --runtime=docker --config=$HOME/.config/docker/daemon.json</p> <p>systemctl --user restart docker</p> <p>sudo nvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place</p> <p>sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi</p>"},{"location":"cuda/cuda_docker.html","title":"cuda docker","text":"<ul> <li> <p>CUDA - Docker</p> <ul> <li>Add daemon.json to /etc/docker</li> <li> <p>https://github.com/dwani-ai/docs/blob/main/files/daemon.json</p> </li> <li> <p>sudo systemctl restart docker</p> </li> <li> <p>sudo docker run --runtime nvidia -it --rm -p 9000:9000 dwani-ai/vllm-arm64</p> </li> <li> <p>vllm serve </p> </li> </ul> </li> </ul>"},{"location":"discovery/index.html","title":"readme","text":"<ul> <li>Discovery Desktop <ul> <li>desktop.md</li> </ul> </li> <li> <p>For Inference use vLLM or llama.cpp </p> <ul> <li> <p>For Server </p> <ul> <li>vllm_deploy.md</li> </ul> </li> <li> <p>For PC</p> <ul> <li>llama-cpp.md </li> </ul> </li> </ul> </li> <li> <p>Discovery Server</p> <ul> <li>discovery_setup.md</li> </ul> </li> <li> <p>Discovery UX</p> <ul> <li>discovery_ux.md</li> </ul> </li> <li> <p>Proxy Server Setup - for https</p> <ul> <li>proxy_setup_vm.md</li> </ul> </li> </ul>"},{"location":"discovery/api_server_setup.html","title":"API Server","text":""},{"location":"discovery/api_server_setup.html#dwani-api-server","title":"dwani-api-server","text":""},{"location":"discovery/api_server_setup.html#git-clone-httpsgithubcomdwani-aidwani-api-servergit-cd-dwani-api-server-python-m-venv-venv-source-venvbinactivate-pip-install-r-requirementstxt-uvicorn-srcservermainapp-host-0000-port-18888","title":"<pre><code>git clone https://github.com/dwani-ai/dwani-api-server.git\n\ncd dwani-api-server\n\npython -m venv venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n\nuvicorn src.server.main:app --host 0.0.0.0 --port 18888 \n</code></pre>","text":"<p>For Docker</p> <pre><code>docker build -t dwani/api-server-arm64:latest -f Dockerfile .\n\ndocker run --env-file .env.server -p 80:80 dwani/api-server-arm64:latest\n</code></pre>"},{"location":"discovery/desktop.html","title":"Dwani.ai - Desktop","text":"<p>Use Dwani.ai on your PC with our dedicated desktop application. </p>"},{"location":"discovery/desktop.html#download-the-app","title":"Download the App","text":"<ul> <li>Linux: Download Dwani Desktop for Linux</li> <li>Windows: Download Dwani Desktop for Windows</li> </ul>"},{"location":"discovery/desktop.html#installation-instructions","title":"Installation Instructions","text":""},{"location":"discovery/desktop.html#for-windows","title":"For Windows","text":"<ul> <li>Windows Installation Guide.</li> </ul>"},{"location":"discovery/desktop.html#for-linux","title":"For Linux","text":"<ul> <li>Linux Installation Guide.</li> </ul>"},{"location":"discovery/discovery_setup.html","title":"Discovery Server","text":""},{"location":"discovery/discovery_setup.html#discovery","title":"Discovery","text":"<ul> <li>For server <pre><code>sudo apt-get update\nsudo apt-get install poppler-utils -y\n\ngit clone https://github.com/dwani-ai/discovery.git\n\ncd discovery\n\npython3.10 -m venv venv\nsource venv/bin/activate\n\npip install -r server-requirements.txt\n\nexport VLLM_IP=\"0.0.0.0\"\nuvicorn server.main:app --host 0.0.0.0 --port 18889\n</code></pre></li> <li> <p>For Local PC <pre><code>sudo apt-get install tesseract-ocr\n\nuvicorn server.local_main:app --host 0.0.0.0 --port 18889\n</code></pre></p> </li> <li> <p>For Client</p> </li> </ul> <pre><code>pip install -r client-requirements.txt\n\npython ux/ux.py\n</code></pre>"},{"location":"discovery/discovery_ux.html","title":"Discovery ux","text":"<p>Discovery UX</p> <ul> <li>With Docker</li> </ul> <pre><code>export VLLM_IP=\"0.0.0.0\"\n\ndocker run -p 80:8000 --env VLLM_IP=$VLLM_IP dwani/discovery_ux:latest\n</code></pre> <ul> <li>With python <pre><code>git clone https://github.com/dwani-ai/discovery.git\n\ncd discovery\n\npython3.10 -m venv venv\nsource venv/bin/activate\npip install client-requirements.txt\npython ux/ux.py\n</code></pre></li> </ul>"},{"location":"discovery/linux_installation.html","title":"Using Dwani.ai on Linux","text":"<p>This guide explains how to download, install, and run the Dwani.ai desktop application on Linux. You can either use the pre-built AppImage or build the application from source.</p>"},{"location":"discovery/linux_installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System: A 64-bit Linux distribution (e.g., Ubuntu 20.04 or later, Fedora, Debian, etc.)</li> <li>Disk Space: At least 500 MB of free disk space</li> <li>Internet Connection: Required for downloading the application or dependencies</li> <li>Node.js (optional, for building from source): Version 16 or higher</li> <li>Git (optional, for cloning the repository)</li> </ul>"},{"location":"discovery/linux_installation.html#option-1-download-and-run-the-pre-built-appimage","title":"Option 1: Download and Run the Pre-Built AppImage","text":"<ol> <li>Download the AppImage:</li> <li>Visit the Dwani.ai GitHub Releases page.</li> <li> <p>Download the Linux AppImage: <code>dwani-desktop-0.0.2.AppImage</code>.</p> </li> <li> <p>Make the AppImage Executable:</p> </li> <li> <p>Open a terminal and navigate to the directory containing the downloaded AppImage (e.g., <code>~/Downloads</code>):      <pre><code>cd ~/Downloads\nchmod +x dwani-desktop-0.0.2.AppImage\n</code></pre></p> </li> <li> <p>Run the Application:</p> </li> <li>Execute the AppImage to launch Dwani.ai:      <pre><code>./dwani-desktop-0.0.2.AppImage\n</code></pre></li> <li>Alternatively, double-click the AppImage file in your file manager if it supports AppImage execution.</li> </ol>"},{"location":"discovery/linux_installation.html#option-2-build-and-run-from-source","title":"Option 2: Build and Run from Source","text":"<p>If you prefer to build the application yourself, follow these steps:</p>"},{"location":"discovery/linux_installation.html#prerequisites-for-building","title":"Prerequisites for Building","text":"<ul> <li>Install Node.js (version 16 or higher).</li> <li>Install Git to clone the repository.</li> <li>Ensure you have <code>npm</code> (comes with Node.js) installed.</li> <li>Install dependencies for Electron Builder (e.g., <code>libfuse2</code> for AppImage support):   <pre><code>sudo apt-get install libfuse2  # For Debian/Ubuntu-based systems\nsudo dnf install fuse-libs    # For Fedora-based systems\n</code></pre></li> </ul>"},{"location":"discovery/linux_installation.html#steps-to-build","title":"Steps to Build","text":"<ol> <li> <p>Clone the Repository:    Open a terminal and run:    <pre><code>git clone https://github.com/dwani-ai/dwani-desktop.git\ncd dwani-desktop/frontend\n</code></pre></p> </li> <li> <p>Install Dependencies:    Install the required Node.js packages:    <pre><code>npm install\n</code></pre></p> </li> <li> <p>Build the Application:    Build the application for Linux:    <pre><code>npx electron-builder --linux\n</code></pre>    This will generate the Linux AppImage in the <code>dist</code> folder.</p> </li> <li> <p>Run the Application:</p> </li> <li>Navigate to the <code>dist</code> folder:      <pre><code>cd dist\n</code></pre></li> <li>Make the AppImage executable:      <pre><code>chmod +x dwani-desktop-0.0.2.AppImage\n</code></pre></li> <li>Run the AppImage:      <pre><code>./dwani-desktop-0.0.2.AppImage\n</code></pre></li> </ol>"},{"location":"discovery/linux_installation.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>AppImage Won\u2019t Run: Ensure the AppImage is executable (<code>chmod +x</code>). If it fails, check for missing dependencies like <code>libfuse2</code>.</li> <li>Build Errors: Verify that Node.js and npm are correctly installed by running <code>node -v</code> and <code>npm -v</code>. Ensure all build dependencies are installed.</li> <li>Permission Issues: Run the AppImage or build commands with appropriate permissions (avoid using <code>sudo</code> unless necessary).</li> <li>For further assistance, visit the Dwani.ai GitHub Issues page or contact the support team.</li> </ul>"},{"location":"discovery/linux_installation.html#additional-notes","title":"Additional Notes","text":"<ul> <li>The pre-built AppImage is recommended for most users as it simplifies the installation process.</li> <li>AppImages are portable and do not require installation, but you may need to install <code>libfuse2</code> on newer Linux distributions.</li> <li>Keep your application updated by checking the GitHub Releases page for the latest version.</li> </ul>"},{"location":"discovery/proxy_setup_vm.html","title":"Proxy Server","text":"<p>Docker Setup</p> <p>https://docs.docker.com/engine/install/</p> <pre><code>for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre>"},{"location":"discovery/proxy_setup_vm.html#add-dockers-official-gpg-key","title":"Add Docker's official GPG key:","text":"<pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n</code></pre>"},{"location":"discovery/proxy_setup_vm.html#add-the-repository-to-apt-sources","title":"Add the repository to Apt sources:","text":"<pre><code>echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <pre><code>sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <pre><code>sudo docker run hello-world\n\n\nsudo groupadd docker\n\nsudo usermod -aG docker $USER\n\nnewgrp docker\n\ndocker run hello-world\n\n\n\ndocker run --env-file .env -p 80:80 dwani/proxy-server:latest\n</code></pre> <ul> <li>Build Proxy Server  <pre><code>docker build -t dwani/proxy-server:latest -f Dockerfile .\n\n\ndocker push dwani/proxy-server:latest\n</code></pre></li> </ul> <p>Proxy Server</p> <pre><code>git clone https://github.com/dwani-ai/proxy-server.git\ncd proxy-server\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\npython src/server/main.py\n</code></pre> <p>For Load balancer </p> <pre><code>git clone https://github.com/dwani-ai/proxy-server.git\ncd proxy-server\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\npython src/server/load_balancer.py\n</code></pre>"},{"location":"discovery/vllm_deploy.html","title":"vLLM Server","text":""},{"location":"discovery/vllm_deploy.html#installation-steps","title":"Installation Steps","text":"<ul> <li>vLLM for arm64/GH200 </li> </ul> <pre><code>sudo apt update\nsudo apt upgrade -y\n\nsudo apt install python3.12 python3.12-venv python3.12-dev poppler-utils -y\n\n\npython3.12 -m venv venv\nsource venv/bin/activate\n\npip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128\n\n\npip install https://github.com/dwani-ai/vllm-arm64/releases/download/v.0.0.4/vllm-0.10.1.dev0+g6d8d0a24c.d20250726-cp312-cp312-linux_aarch64.whl\n\n\n\nvllm serve RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 98304 --disable-log-requests --dtype bfloat16 --enable-chunked-prefill --enable-prefix-caching --max-num-batched-tokens 8192 --chat-template-content-format openai\n</code></pre>"},{"location":"discovery/windows_installation.html","title":"Using Dwani.ai on Windows","text":"<p>This guide explains how to download, install, and run the Dwani.ai desktop application on Windows. You can either use the pre-built executable or build the application from source.</p>"},{"location":"discovery/windows_installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System: Windows 10 or later (64-bit)</li> <li>Disk Space: At least 500 MB of free disk space</li> <li>Internet Connection: Required for downloading the application or dependencies</li> <li>Node.js (optional, for building from source): Version 16 or higher</li> <li>Git (optional, for cloning the repository)</li> </ul>"},{"location":"discovery/windows_installation.html#option-1-download-and-install-the-pre-built-application","title":"Option 1: Download and Install the Pre-Built Application","text":"<ol> <li>Download the Installer:</li> <li>Visit the Dwani.ai GitHub Releases page.</li> <li> <p>Download the Windows executable: <code>dwani-desktop.Setup.0.0.2.exe</code>.</p> </li> <li> <p>Run the Installer:</p> </li> <li>Locate the downloaded <code>dwani-desktop.Setup.0.0.2.exe</code> file in your Downloads folder.</li> <li>Double-click the file to start the installation process.</li> <li> <p>Follow the on-screen prompts to complete the installation.</p> </li> <li> <p>Launch the Application:</p> </li> <li>Once installed, find the Dwani.ai application in your Start Menu or on your Desktop (if a shortcut was created).</li> <li>Double-click the Dwani.ai icon to launch the application.</li> </ol>"},{"location":"discovery/windows_installation.html#option-2-build-and-run-from-source","title":"Option 2: Build and Run from Source","text":"<p>If you prefer to build the application yourself, follow these steps:</p>"},{"location":"discovery/windows_installation.html#prerequisites-for-building","title":"Prerequisites for Building","text":"<ul> <li>Install Node.js (version 16 or higher).</li> <li>Install Git to clone the repository.</li> <li>Ensure you have <code>npm</code> (comes with Node.js) installed.</li> </ul>"},{"location":"discovery/windows_installation.html#steps-to-build","title":"Steps to Build","text":"<ol> <li> <p>Clone the Repository:    Open a terminal (e.g., Command Prompt, PowerShell, or Git Bash) and run:    <pre><code>git clone https://github.com/dwani-ai/dwani-desktop.git\ncd dwani-desktop/frontend\n</code></pre></p> </li> <li> <p>Install Dependencies:    Install the required Node.js packages:    <pre><code>npm install\n</code></pre></p> </li> <li> <p>Build the Application:    Build the application for Windows:    <pre><code>npx electron-builder --win\n</code></pre>    This will generate the Windows executable in the <code>dist</code> folder.</p> </li> <li> <p>Run the Application:</p> </li> <li>Navigate to the <code>dist</code> folder:      <pre><code>cd dist\n</code></pre></li> <li>Locate the generated <code>dwani-desktop.Setup.0.0.2.exe</code> (version may vary).</li> <li>Double-click the <code>.exe</code> file to install and run the application, or run it directly from the <code>dist</code> folder if an unpacked version is available (e.g., <code>dwani-desktop.exe</code>).</li> </ol>"},{"location":"discovery/windows_installation.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Installation Fails: Ensure you have administrative privileges on your Windows account.</li> <li>Application Won\u2019t Start: Check that your system meets the prerequisites and that the <code>.exe</code> file is not blocked by your antivirus software.</li> <li>Build Errors: Verify that Node.js and npm are correctly installed by running <code>node -v</code> and <code>npm -v</code> in your terminal.</li> <li>For further assistance, visit the Dwani.ai GitHub Issues page or contact the support team.</li> </ul>"},{"location":"discovery/windows_installation.html#additional-notes","title":"Additional Notes","text":"<ul> <li>The pre-built executable is recommended for most users as it simplifies the installation process.</li> <li>If you encounter issues with the source build, ensure your Node.js and npm versions are compatible.</li> <li>Keep your application updated by checking the GitHub Releases page for the latest version.</li> </ul>"},{"location":"examples/index.html","title":"readme","text":"<p>Projects built with dwani.ai</p> <ul> <li> <p>Legal Aliens - XR game - https://github.com/sachinsshetty/xr_dwani</p> <ul> <li>Video Demo<ul> <li>Puzzle generation</li> <li>Object Detection + Bounding Box</li> </ul> </li> </ul> </li> <li> <p>Smart City - https://github.com/sachinsshetty/smart_city_hackathon</p> <ul> <li>Video demo <ul> <li>Vision Assistance with Voice Interaction</li> <li>Object Detection/ Image Description</li> <li>Automatic Speech Recognition/ ASR</li> <li>Speech Synthesis / Text to Speech</li> </ul> </li> </ul> </li> <li> <p>Biryani Bot - https://github.com/sachinsshetty/biryani_bot</p> </li> <li> <p>llm-recipes - https://github.com/slabstech/llm-recipes</p> </li> </ul>"},{"location":"local/index.html","title":"readme","text":"<p>Discovery Lite</p> <ul> <li> <p>Install llama.cpp</p> </li> <li> <p>Setup moondream for VLM</p> </li> <li> <p>TODO</p> <ul> <li>gpt-oss-20b - LLM</li> <li>moondream - VLM</li> <li>whisper - ASR</li> <li>kokoro - TTS</li> <li>gemma3-270M  - fine-tuning</li> </ul> </li> </ul>"},{"location":"local/llama-cpp.html","title":"llama.cpp","text":"<p>llama.cpp for for Discovery</p> <ul> <li>Setup llama.cpp  <pre><code>sudo apt install libcurl4-openssl-dev\n\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\n</code></pre><ul> <li>With NVIDIA GPU and CUDA Toolkit <pre><code>cmake -B build -DGGML_CUDA=ON\n</code></pre></li> <li>for CPU  <pre><code>cmake -B build\n</code></pre></li> </ul> </li> <li> <p>Create Executable <pre><code>cmake --build build --config Release -j4\n</code></pre></p> </li> <li> <p>To run Gemma-3-4b-IT <pre><code>./build/bin/llama-server -hf ggml-org/gemma-3-4b-it-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 8192 --alias gemma3\n</code></pre></p> </li> <li> <p>To - OpenAI - gpt-oss-20b</p> <ul> <li>For Laptop / PC -  gpt-oss-20b <pre><code>./build/bin/llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none --port 9500\n</code></pre></li> <li>For H100/H200 - gpt-oss-120b <pre><code>./build/bin/llama-server -hf ggml-org/gpt-oss-120b-GGUF -c 0 -fa --jinja --reasoning-format none --port 9500\n</code></pre></li> </ul> </li> <li> <p>Then, access http://localhost:8080</p> </li> <li> <p>Reference </p> <ul> <li>https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/</li> <li>HF model repo - https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf</li> </ul> </li> </ul>"},{"location":"local/moondream.html","title":"moondream","text":"<p>moondream</p> <pre><code> python3.10 -m venv venv\n source venv/bin/activate\n\npip install transformers torch pillow einops\n\npip install pyvips-binary pyvips accelerate\n</code></pre> <p>https://moondream.ai/c/docs/advanced/api/caption</p>"},{"location":"misc/index.html","title":"readme","text":"<p>Everythin Else</p> <ul> <li>arm64:<ul> <li>readme : arm64/README.md</li> <li>vLLM wheel : arm64/vllm-setup.md</li> <li>vLLM docker build : arm64/vllm_docker_build.md</li> <li>xfomers : arm64/xfomers.md </li> <li>flashinfer : arm64/flashinfer.md</li> <li>torch: arm64/torch_2_8.md</li> </ul> </li> <li>local:<ul> <li>readme : local/index.md</li> <li>llama.cpp : local/llama-cpp.md</li> <li>moondream : local/moondream.md</li> </ul> </li> <li>cuda :<ul> <li>readme : cuda/index.md</li> <li>cuda driver : cuda/cuda_12_9-setup.md</li> <li>cuda docker : cuda/cuda_docker.md</li> </ul> </li> <li>mobile :<ul> <li>readme : mobile/index.md</li> <li>android : mobile/android-app.md</li> </ul> </li> <li>todo:<ul> <li>arm64 upgrade : todo/todo_arm_64_arm64-gpu-tools.md</li> </ul> </li> </ul>"},{"location":"misc/docker-vllm-build.html","title":"Docker vllm build","text":"<p>Docker Container for vllm</p> <p><pre><code>python3.12 -m venv venv\nsource venv/bin/activate\npip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128\n\n\n\nsudo /etc/docker/daemon.json\n</code></pre> copy file contents from daemon.json </p> <p><pre><code> sudo systemctl restart docker\n\n export INDEX_HOST=jetson-ai-lab.io\n\n\n\ngit clone https://github.com/sachinsshetty/jetson-containers\nbash jetson-containers/install.sh\n\nCUDA_VERSION=12.8 jetson-containers build pytorch\n\nCUDA_VERSION=12.8 jetson-containers build vllm\n\n\njetson-containers build vllm\n</code></pre>   - sudo docker tag vllm:r36.4-cu128-24.04-flashinfer vllm:latest</p> <p>sudo docker build -t dwani/vllm-arm64:latest -f Dockerfile .</p> <p>sudo docker push dwani/vllm-arm64:latest</p> <p>sudo docker run --runtime nvidia -it --rm --network=host dwani:vllm</p> <p>sudo docker run --runtime nvidia -it --rm -p 7890:8000 dwani:vllm</p> <p>--</p> <p>docker pull dwani/vllm-arm64:latest sudo docker tag dwani/vllm-arm64:latest dwani/vllm-arm64:latest</p> <p>sudo docker push dwani/vllm-arm64:latest</p> <ul> <li>References</li> <li>https://github.com/sachinsshetty/jetson-containers/blob/master/docs/setup.md</li> </ul>"},{"location":"misc/dwani-modules.html","title":"Dwani modules","text":"<p>dwani.ai - gh200 Setup</p>"},{"location":"misc/dwani-modules.html#export-api_key_secretdwani-mobile-app-some-sercwer234-export-chat_rate_limit100minute-export-dwani_api_base_url_pdfhttp1270017861-export-dwani_api_base_url_visionhttp1270017861-export-dwani_api_base_url_llmhttp1270017861-export-dwani_api_base_url_ttshttp1270017864-export-dwani_api_base_url_asrhttp1270017863-export-dwani_api_base_url_translatehttp1270017862-export-dwani_api_base_url_s2shttp1270017861-export-speech_rate_limit5minute-export-encryption_keytetegdgfdgfdfgdfgdfg-export-default_admin_useradminsdfsdf-export-default_admin_passworddwani-987-123234fsfsfsfsfd-export-hf_tokenhf_this_is_not_a_secret__this_gaganyatri","title":"<pre><code>export API_KEY_SECRET=\"dwani-mobile-app-some-sercwer234\"\nexport CHAT_RATE_LIMIT=\"100/minute\"\nexport DWANI_API_BASE_URL_PDF=\"http://127.0.0.1:7861\"\nexport DWANI_API_BASE_URL_VISION=\"http://127.0.0.1:7861\"\nexport DWANI_API_BASE_URL_LLM=\"http://127.0.0.1:7861\"\nexport DWANI_API_BASE_URL_TTS=\"http://127.0.0.1:7864\"\nexport DWANI_API_BASE_URL_ASR=\"http://127.0.0.1:7863\"\nexport DWANI_API_BASE_URL_TRANSLATE=\"http://127.0.0.1:7862\"\nexport DWANI_API_BASE_URL_S2S=\"http://127.0.0.1:7861\"\nexport SPEECH_RATE_LIMIT=\"5/minute\"\nexport ENCRYPTION_KEY=\"tetegdgfdgfdfgdfgdfg\"\nexport DEFAULT_ADMIN_USER=\"adminsdfsdf\"\nexport DEFAULT_ADMIN_PASSWORD=\"dwani-987-123234fsfsfsfsfd\"\n\nexport HF_TOKEN='hf_this_is_not_a_secret__this_gaganyatri'\n</code></pre>","text":"<p>pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128</p> <ul> <li>https://github.com/dwani-ai/dwani-api-server <pre><code>git clone https://github.com/dwani-ai/dwani-api-server\ncd dwani-api-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n\npython src/server/main.py --host  0.0.0.0 --port 8888\n</code></pre></li> </ul> <ul> <li> </li> </ul> <ul> <li> </li> <li>https://github.com/dwani-ai/indic-translate-server.git <pre><code>git clone https://github.com/dwani-ai/indic-translate-server.git\ncd indic-translate-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\npip install --upgrade pip setuptools wheel packaging cython\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\n#pip install -r requirements.txt\n\npip install -e git+https://github.com/VarunGumma/IndicTransToolkit.git@main#egg=IndicTransToolkit\n\npip install fastapi uvicorn  \"numpy&lt;2.0\"\npython src/server/translate_api.py --host 0.0.0.0 --port 7862 --device cuda\n</code></pre></li> </ul> <ul> <li> </li> <li> <p>https://github.com/dwani-ai/docs-indic-server.git <pre><code>git clone https://github.com/dwani-ai/docs-indic-server.git\ncd docs-indic-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\npip install \"numpy&lt;2.0\"\n\npython src/server/docs_api.py  --host 0.0.0.0 --port 7861\n</code></pre></p> </li> <li> <p>Dependencies</p> </li> <li>decord</li> </ul> <pre><code>cd\nmkdir external\ncd external\ngit clone --recursive https://github.com/dmlc/decord\n\ncd decord\n\nmkdir build &amp;&amp; cd build\n\ncmake .. -DUSE_CUDA=0 -DCMAKE_BUILD_TYPE=Release\n\nmake\n\ncd ../python\npython3 setup.py install --user\npip install \"numpy&lt;2.0\"\n</code></pre> <ul> <li>olmocr</li> </ul> <p><pre><code>cd ../../\ngit clone  https://github.com/allenai/olmocr.git\n\ncd olmocr\n\npip install --upgrade pip setuptools wheel packaging\n\n // copy ppyproject.toml\npip install -e .\ncd ../../dwani_org/gh-200-docs-indic-server/\npip install \"numpy&lt;2.0\"\n</code></pre> in olmocr :  pyproject.toml - remove sql-kernem and sglang set python version to 3.10</p> <p>--</p> <ul> <li> </li> <li>https://github.com/dwani-ai/tts-indic-server <pre><code>git clone https://github.com/dwani-ai/tts-indic-server\ncd tts-indic-server\ngit checkout gh-200\nexport HF_TOKEN='this-my-token'\npython -m venv  venv\nsource venv/bin/activate\npip install wheel packaging\n\npip install -r requirements.txt\n\n pip uninstall torch torchaudio torchvision\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\npython src/gh200/main.py --host 0.0.0.0 --port 7864 --config config_two\n</code></pre></li> </ul>"},{"location":"misc/dwani-modules.html#api-server","title":"API server","text":""},{"location":"misc/dwani-modules.html#asr-automatic-speech-recognition-speech-to-text","title":"ASR - Automatic Speech Recognition / Speech to Text","text":"<ul> <li>https://github.com/dwani-ai/asr-indic-server.git <pre><code>git clone https://github.com/dwani-ai/asr-indic-server.git\ncd asr-indic-server\n\npython -m venv --system-site-packages venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n\npython src/multilingual/asr_api.py --host 0.0.0.0 --port 7863 --device cuda\n</code></pre></li> </ul>"},{"location":"misc/dwani-modules.html#translate","title":"Translate","text":""},{"location":"misc/dwani-modules.html#documents","title":"Documents","text":""},{"location":"misc/dwani-modules.html#tts-text-to-speech","title":"TTS - Text to Speech","text":""},{"location":"misc/evals.html","title":"Evals","text":"<p>Evals </p> <p>https://eval.16x.engineer/blog/kimi-k2-provider-evaluation-results</p>"},{"location":"misc/gemm3.html","title":"Gemm3","text":"<p>Using a slow image processor as <code>use_fast</code> is unset and a slow processor was saved with this model. <code>use_fast=True</code> will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with <code>use_fast=False</code>.</p>"},{"location":"misc/google_cloud.html","title":"Google cloud","text":"<p>Google - VM Setup</p> <p>sudo apt install nano git -y</p> <p>sudo    apt install python3.12-venv -y</p> <p>git clone https://github.com/dwani-ai/dwani-api-server.git</p> <p>cd dwani-api-server  python3 -m venv venv  source venv/bin/activate pip install --upgrade pip pip install setuptools-rust</p> <p>pip install -r requirements.txt</p> <p>sudo python src/server/main.py --host  0.0.0.0 --port 80</p> <p>export API_KEY_SECRET=\"dwani-moasdsbile-app\" export CHAT_RATE_LIMIT=\"100/minute\" export DWANI_API_BASE_URL_PDF=\"http://127.0.0.1:7861\" export DWANI_API_BASE_URL_VISION=\"http://127.0.0.1:7861\" export DWANI_API_BASE_URL_LLM=\"http://127.0.0.1:7861\" export DWANI_API_BASE_URL_LLM_QWEN=\"http://127.0.0.1:7880\" export DWANI_API_BASE_URL_TTS=\"http://127.0.0.1:7864\" export DWANI_API_BASE_URL_ASR=\"http://127.0.0.1:7863\" export DWANI_API_BASE_URL_TRANSLATE=\"http://127.0.0.1:7862\" export DWANI_API_BASE_URL_S2S=\"http://127.0.0.1:7861\" export SPEECH_RATE_LIMIT=\"5/minute\" export ENCRYPTION_KEY=\"tetasdasde\" export DEFAULT_ADMIN_USER=\"admiasdasdan\" export DEFAULT_ADMIN_PASSWORD=\"dwani-987-123asdasd\"</p>"},{"location":"misc/llama-cpp-setup.html","title":"Llama cpp setup","text":"<p>Gh200 - setup</p> <pre><code>sudo apt-get update\nsudo apt-get install ninja-build\n\nsudo apt-get install libcurl4-openssl-dev\n\nsudo apt-get install -y build-essential python3-dev python3-setuptools make cmake\nsudo apt-get install -y ffmpeg libavcodec-dev libavfilter-dev libavformat-dev libavutil-dev\nsudo apt install -y poppler-utils\nmkdir dwani_org\ncd dwani_org\n\n\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\n\ncmake -B build -DGGML_CUDA=ON\n\ncmake --build build --config Release -j2\n</code></pre> <pre><code>python -m venv --system-site-packages venv\nsource venv/bin/activate\npip install huggingface_hub\nmkdir hf_models \n</code></pre> <ul> <li>gemma3</li> </ul> <pre><code>huggingface-cli download google/gemma-3-27b-it-qat-q4_0-gguf --local-dir hf_models/\n\n ./build/bin/llama-server   --model hf_models/gemma-3-27b-it-q4_0.gguf  --mmproj hf_models/mmproj-model-f16-27B.gguf  --host 0.0.0.0   --port 9000   --n-gpu-layers 100   --threads 4   --ctx-size 4096   --batch-size 256\n</code></pre> <ul> <li>qwen3</li> <li> <p>https://huggingface.co/Qwen/Qwen3-8B-GGUF  - 8.71 <pre><code>huggingface-cli download Qwen/Qwen3-8B-GGUF --local-dir hf_models/\n\n./build/bin/llama-server   --model hf_models/Qwen3-8B-Q8_0.gguf   --host 0.0.0.0   --port 7880 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256\n</code></pre></p> </li> <li> <p>https://huggingface.co/Qwen/Qwen3-14B-GGUF - 15 GB <pre><code>huggingface-cli download Qwen/Qwen3-14B-GGUF --local-dir hf_models/\n\n./build/bin/llama-server   --model hf_models/Qwen3-14B-Q8_0.gguf   --host 0.0.0.0   --port 7880 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256\n</code></pre></p> </li> <li> <p>https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF - 32 GB</p> </li> </ul> <pre><code>huggingface-cli download Qwen/Qwen3-30B-A3B-GGUF --local-dir hf_models/\n\n./build/bin/llama-server   --model hf_models/Qwen3-30B-A3B-Q8_0.gguf   --host 0.0.0.0   --port 7880 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256\n</code></pre> <ul> <li> <p>moondream <pre><code>huggingface-cli download ggml-org/moondream2-20250414-GGUF --local-dir hf_models/\n\n./build/bin/llama-server   --model hf_models/moondream2-text-model-f16_ct-vicuna.gguf --mmproj hf_models/moondream2-mmproj-f16-20250414.gguf --host 0.0.0.0 --port 9000   --n-gpu-layers 100   --threads 4   --ctx-size 4096   --batch-size 256\n</code></pre></p> </li> <li> <p>sarvam-m</p> </li> </ul> <pre><code>huggingface-cli download sarvamai/sarvam-m-q8-gguf --local-dir hf_models/\n\n./build/bin/llama-server   --model hf_models/sarvam-m-q8_0.gguf --host 0.0.0.0 --port 7884 --n-gpu-layers 100   --threads 4   --ctx-size 4096   --batch-size 256\n</code></pre> <p>curl -X POST http://localhost:7860/v1/chat/completions\\   -H \"Content-Type: application/json\" \\   -d '{     \"model\": \"gemma-3-12b-it\",     \"messages\": [       {\"role\": \"user\", \"content\": \"Hello, how are you?\"}     ],     \"max_tokens\": 100,     \"temperature\": 0.7   }'</p> <p>curl -X POST https://abcd.hf.space/v1/chat/completions\\   -H \"Content-Type: application/json\" \\   -d '{     \"model\": \"gemma-3-12b-it\",     \"messages\": [       {\"role\": \"user\", \"content\": \"Hello, how are you?\"}     ],     \"max_tokens\": 100,     \"temperature\": 0.7   }'</p> <ul> <li> <p>https://docs.lambda.ai/education/running-huggingface-diffusers-transformers-gh200/</p> </li> <li> <p>https://lambda.ai/blog/putting-the-nvidia-gh200-grace-hopper-superchip-to-good-use-superior-inference-performance-and-economics</p> </li> <li> <p>https://docs.lambda.ai/education/fine-tune-mochi-gh200/</p> </li> <li> <p>https://docs.lambda.ai/public-cloud/on-demand/troubleshooting/#why-lambdas-gh200-specifications-differ-from-nvidias</p> </li> </ul> <p>pixtral</p> <p>huggingface-cli download bartowski/mistral-community_pixtral-12b-GGUF --include \"mistral-community_pixtral-12b-Q4_K_M.gguf\" --local-dir hf_models</p> <p>huggingface-cli download bartowski/mistral-community_pixtral-12b-GGUF --include \"mistral-community_pixtral-12b-Q8_0.gguf\" --local-dir hf_models</p> <p>./build/bin/llama-server --model hf_models/mistral-community_pixtral-12b-Q8_0.gguf --mmproj hf_models/pixtral-12b-mmproj.gguf --host 0.0.0.0 --port 9000 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256</p> <p>./build/bin/llama-server --model hf_models/mistral-community_pixtral-12b-Q8_0.gguf  --host 0.0.0.0 --port 9000 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256</p> <p>./build/bin/llama-server -hf ggml-org/pixtral-12b-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256</p> <p>./build/bin/llama-server -hf ggml-org/Qwen2.5-VL-32B-Instruct-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 100 --threads 4 --ctx-size 4096 --batch-size 256</p> <p>https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal.md</p>"},{"location":"misc/setup.html","title":"Setup","text":"<p>Setup</p> <ul> <li> <p>vllm + gemma3 Setup</p> <ul> <li>Add daemon.json to /etc/docker</li> <li> <p>https://github.com/dwani-ai/deploy/blob/main/files/daemon.json</p> </li> <li> <p>sudo systemctl restart docker</p> </li> <li> <p>sudo docker run --runtime nvidia -it --rm -p 9000:9000 slabstech/dwani-vllm</p> </li> <li> <p>export HF_TOKEN='my-nsma-is-what'</p> </li> <li> <p>vllm serve google/gemma-3-4b-it --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 16384     --dtype bfloat16 </p> </li> </ul> </li> <li> <p>docs-indic-server   -- ### Documents </p> </li> <li> <p>https://github.com/dwani-ai/docs-indic-server.git <pre><code>git clone https://github.com/dwani-ai/docs-indic-server.git\ncd docs-indic-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\npip install \"numpy&lt;2.0\"\n\npython src/server/docs_api.py  --host 0.0.0.0 --port 7861\n</code></pre></p> </li> <li> <p>Dependencies</p> </li> <li>decord</li> <li>olmocr</li> <li> <p>ffmpeg</p> </li> <li> <p>ffmpeg</p> </li> <li>sudo apt update</li> <li>sudo apt install ffmpeg</li> <li>ffmpeg -version</li> </ul> <p>sudo apt update sudo apt install cmake ffmpeg build-essential python3-dev</p> <p>sudo apt install pkg-config libavcodec-dev libavformat-dev libavutil-dev libswscale-dev libswresample-dev  libavfilter-dev </p> <p>sudo apt-get install poppler-utils</p> <p>find /usr/lib* -name libavformat.so</p> <p>sudo ln -s /usr/lib/aarch64-linux-gnu/libavformat.so /usr/lib/libavformat.so sudo ln -s /usr/lib/aarch64-linux-gnu/libavcodec.so /usr/lib/libavcodec.so sudo ln -s /usr/lib/aarch64-linux-gnu/libavutil.so /usr/lib/libavutil.so sudo ln -s /usr/lib/aarch64-linux-gnu/libavfilter.so /usr/lib/libavfilter.so sudo ln -sf /usr/lib/aarch64-linux-gnu/libavdevice.so.58 /usr/lib/libavdevice.so</p> <p>sudo ln -s /usr/lib/aarch64-linux-gnu/libswresample.so  /usr/lib/libswresample.so</p> <p>sudo ldconfig</p>"},{"location":"misc/setup.html#add-other-ffmpeg-libs-as","title":"Add other FFmpeg libs as","text":"<p>cmake .. -DUSE_CUDA=1 -DCMAKE_BUILD_TYPE=Release -DFFMPEG_LIB_DIR=/usr/lib/aarch64-linux-gnu -DFFMPEG_INCLUDE_DIR=/usr/include</p> <pre><code>cd\nmkdir external\ncd external\ngit clone --recursive https://github.com/dmlc/decord\n\ncd decord\n\nmkdir build &amp;&amp; cd build\n\nexport CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\nexport CUDACXX=/usr/local/cuda/bin/nvcc\n\nnvcc --version\n\ncmake .. -DUSE_CUDA=1 -DCMAKE_BUILD_TYPE=Release -DFFMPEG_DIR=/usr\n\ncmake .. -DUSE_CUDA=1 -DCMAKE_BUILD_TYPE=Release\n\nmake\n\ncd ../python\npython3 setup.py install --user\npip install \"numpy&lt;2.0\"\n</code></pre> <ul> <li>olmocr</li> </ul> <p><pre><code>cd ../../\ngit clone  https://github.com/allenai/olmocr.git\n\ncd olmocr\n\npip install --upgrade pip setuptools wheel packaging\n\n // copy ppyproject.toml\npip install -e .\ncd ../../dwani_org/gh-200-docs-indic-server/\npip install \"numpy&lt;2.0\"\n</code></pre> in olmocr :  pyproject.toml - remove sql-kernem and sglang set python version to 3.10</p> <p>python src/server/docs_api.py  --host 0.0.0.0 --port 7861</p> <p>--</p> <ul> <li>dwani-api-server</li> </ul> <p>git clone https://github.com/dwani-ai/dwani-api-server.git cd dwani-api-server</p> <p>python -m venv venv source venv/bin/activate</p> <p>pip install -r requirements.txt docker build -t dwani/api-server-arm64:latest -f Dockerfile .</p> <p>Proxy Server</p> <p>sudo apt install nano Install docker in VM - </p> <p>docker build -t dwani/proxy-server:latest -f Dockerfile .</p> <p>docker push dwani/proxy-server:latest</p> <p>docker run --env-file .env -p 80:80 dwani/proxy-server:latest</p> <p>--</p> <p>Translate Server</p> <ul> <li> <p>https://github.com/dwani-ai/indic-translate-server.git <pre><code>git clone https://github.com/dwani-ai/indic-translate-server.git\ncd indic-translate-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\npip install --upgrade pip setuptools wheel packaging cython\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\n#pip install -r requirements.txt\n\npip install -e git+https://github.com/VarunGumma/IndicTransToolkit.git@main#egg=IndicTransToolkit\n\npip install fastapi uvicorn  \"numpy&lt;2.0\"\npython src/server/translate_api.py --host 0.0.0.0 --port 7862 --device cpu\n</code></pre></p> </li> <li> </li> </ul> <p>ASR - English</p> <p>curl --silent --remote-name https://raw.githubusercontent.com/speaches-ai/speaches/master/compose.yaml curl --silent --remote-name https://raw.githubusercontent.com/speaches-ai/speaches/master/compose.cuda.yaml export COMPOSE_FILE=compose.cuda.yaml</p> <p>sudo docker compose -f compose.cuda.yaml up -d</p> <p>curl -X 'POST' \\   'http://localhost:8000/v1/models/Systran/faster-whisper-large-v3' \\   -H 'accept: application/json' \\   -d ''</p> <p>curl -X 'POST' \\   'http://localhost:8000/v1/models/speaches-ai/Kokoro-82M-v1.0-ONNX' \\   -H 'accept: application/json' \\   -d ''</p> <p>-</p> <ul> <li> </li> <li>https://github.com/dwani-ai/tts-indic-server <pre><code>git clone https://github.com/dwani-ai/tts-indic-server\ncd tts-indic-server\ngit checkout gh-200\nexport HF_TOKEN='this-my-token'\npython -m venv  venv\nsource venv/bin/activate\npip install wheel packaging\n\npip install -r requirements.txt\n\n pip uninstall torch torchaudio torchvision\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\npython src/gh200/main.py --host 0.0.0.0 --port 7864 --config config_two\n</code></pre></li> </ul>"},{"location":"misc/setup.html#asr-automatic-speech-recognition-speech-to-text","title":"ASR - Automatic Speech Recognition / Speech to Text","text":"<ul> <li>https://github.com/dwani-ai/asr-indic-server.git <pre><code>git clone https://github.com/dwani-ai/asr-indic-server.git\ncd asr-indic-server\n\npython -m venv --system-site-packages venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n\npython src/multilingual/asr_api.py --host 0.0.0.0 --port 7863 --device cuda\n</code></pre></li> </ul>"},{"location":"misc/setup.html#tts-text-to-speech","title":"TTS - Text to Speech","text":""},{"location":"misc/v2/bug-report.html","title":"Bug report","text":"<p>Bug Report</p> <p>sudo docker run -it --runtime nvidia --gpus all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ -p 8000:8000 \\ --ipc=host \\ --name my_vllm_container \\ dwani/vllm-arm64:latest</p> <p>sudo docker exec -it my_vllm_container /bin/bash</p> <p>vllm serve Qwen/Qwen2.5-1.5B-Instruct</p> <p>vllm serve Qwen/Qwen3-0.6B</p> <p>docker run --runtime nvidia --gpus all \\     -v ~/.cache/huggingface:/root/.cache/huggingface \\     -p 8000:8000 \\     --ipc=host \\     vllm/vllm-openai:latest \\     --model Qwen/Qwen2.5-1.5B-Instruct</p> <p>docker run --runtime nvidia --gpus all \\     -v ~/.cache/huggingface:/root/.cache/huggingface \\     -p 8000:8000 \\     --ipc=host \\     dwani/vllm-new:latest \\     --model Qwen/Qwen2.5-1.5B-Instruct</p> <p>sudo docker run --runtime nvidia --gpus all \\   -v ~/.cache/huggingface:/root/.cache/huggingface \\   -p 8000:8000 \\   --ipc=host \\   dwani/vllm-new:latest \\   python3 -m vllm.entrypoints.api_server --model Qwen/Qwen2.5-1.5B-Instruct</p> <p>sudo docker run --runtime nvidia --gpus all \\     -v ~/.cache/huggingface:/root/.cache/huggingface \\     -p 8000:8000 \\     --ipc=host \\     vllm/vllm-openai:latest \\     --model Qwen/Qwen2.5-1.5B-Instruct Unable to find image 'vllm/vllm-openai:latest' locally latest: Pulling from vllm/vllm-openai Digest: sha256:37cd5bd18d220a0f4c70401ce1d4a0cc588fbfe03cc210579428f2c47e6eac33 Status: Downloaded newer image for vllm/vllm-openai:latest WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested exec /usr/bin/python3: exec format error</p> <p>DOCKER_BUILDKIT=1 docker build . \\   --target vllm-openai \\   --platform \"linux/arm64\" \\   -t vllm/vllm-gh200-openai:latest \\   --build-arg max_jobs=66 \\   --build-arg nvcc_threads=2 \\   --build-arg torch_cuda_arch_list=\"9.0+PTX\" \\   --build-arg vllm_fa_cmake_gpu_arches=\"90-real\"</p> <p>--</p> <p>docker run --runtime nvidia --gpus all \\     -v ~/.cache/huggingface:/root/.cache/huggingface \\     --env \"HUGGING_FACE_HUB_TOKEN=\" \\     -p 8000:8000 \\     --ipc=host \\     vllm/vllm-openai:latest \\     --model Qwen/Qwen2.5-1.5B-Instruct <ul> <li> <p>triton nvcr.io/nvidia/tritonserver:25.06-vllm-python-py3</p> </li> <li> <p>cuda nvcr.io/nvidia/cuda:12.8.1-cudnn-devel-ubuntu22.04</p> </li> <li> <p>pytorch nvcr.io/nvidia/pytorch:25.06-py3</p> </li> <li></li> </ul> <p>sudo docker run --runtime nvidia --gpus all \\   -v ~/.cache/huggingface:/root/.cache/huggingface \\   -p 8000:8000 \\   --ipc=host \\   dwani/vllm-new:latest \\   python3 -m vllm.entrypoints.api_server --model Qwen/Qwen2.5-1.5B-Instruct</p> <p>========== == CUDA == ==========</p> <p>CUDA Version 12.8.1</p> <p>Container image Copyright (c) 2016-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.</p> <p>This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license</p> <p>A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.</p> <p>INFO 07-24 18:18:24 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available. WARNING 07-24 18:18:24 [importing.py:75] Triton is not installed. Using dummy decorators. Install it via <code>pip install triton</code> to enable kernel compilation. INFO 07-24 18:18:26 [init.py:244] Automatically detected platform cuda. Traceback (most recent call last):   File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main     mod_name, mod_spec, code = _get_module_details(mod_name, _Error)   File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details     import(pkg_name)   File \"/usr/local/lib/python3.10/dist-packages/vllm/init.py\", line 13, in      from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py\", line 22, in      from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,   File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 37, in      from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/init.py\", line 4, in      from vllm.model_executor.parameter import (BasevLLMParameter,   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/parameter.py\", line 10, in      from vllm.distributed import get_tensor_model_parallel_rank   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/init.py\", line 4, in      from .communication_op import *   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/communication_op.py\", line 9, in      from .parallel_state import get_tp_group   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 150, in      from vllm.platforms import current_platform   File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/init.py\", line 276, in getattr     _current_platform = resolve_obj_by_qualname(   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2258, in resolve_obj_by_qualname     module = importlib.import_module(module_name)   File \"/usr/lib/python3.10/importlib/init.py\", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)   File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 18, in      import vllm._C  # noqa ImportError: libtorch_cuda.so: cannot open shared object file: No such file or directory <p>pytorch image</p> <p>[3/3] RUN pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v0.0.1/vllm-0.9.2.dev144+g9206d0ff0.d20250618-cp310-cp310-linux_aarch64.whl: 0.420 ERROR: vllm-0.9.2.dev144+g9206d0ff0.d20250618-cp310-cp310-linux_aarch64.whl is not a supported wheel on this platform.</p>"},{"location":"misc/v2/bug-report.html#dockerfilepytorch9","title":"Dockerfile.pytorch:9","text":"<p>7 |       8 |       9 | &gt;&gt;&gt; RUN pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v0.0.1/vllm-0.9.2.dev144+g9206d0ff0.d20250618-cp310-cp310-linux_aarch64.whl   10 |     </p> <p>ERROR: failed to build: failed to solve: process \"/bin/sh -c pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v0.0.1/vllm-0.9.2.dev144+g9206d0ff0.d20250618-cp310-cp310-linux_aarch64.whl\" did not complete successfully: exit code: 1</p>"},{"location":"misc/v2/gpt-oss.html","title":"Gpt oss","text":"<p>GPT OSS - arm64</p> <p>pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v0.0.6/vllm-0.10.1.dev398+g9edd1db02.d20250806-cp312-cp312-linux_aarch64.whl</p> <p>export VLLM_USE_FLASHINFER_MXFP4_BF16_MOE=1</p> <p>export VLLM_USE_TRTLLM_ATTENTION=1 export VLLM_USE_TRTLLM_DECODE_ATTENTION=1 export VLLM_USE_TRTLLM_CONTEXT_ATTENTION=1</p> <p>vllm serve openai/gpt-oss-20b</p> <p>vllm serve openai/gpt-oss-120b</p> <ul> <li>Steps to build  git clone https://github.com/vllm-project/vllm.git</li> </ul> <p>cd vllm</p> <p>pip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128</p> <p>python use_existing_torch.py </p> <p>pip install --upgrade setuptools twine setuptools-scm</p> <p>pip install -r requirements/cuda.txt</p> <p>export MAX_JOBS=16 export NVCC_THREADS=4 export TORCH_CUDA_ARCH_LIST=\"\" export VLLM_TARGET_DEVICE=cuda</p> <p>python setup.py bdist_wheel</p> <p>pip install dist/*.whl</p>"},{"location":"misc/v2/gpt-oss.html#gptossforcausallm","title":"GptOssForCausalLM","text":"<p>https://blog.vllm.ai/2025/08/05/gpt-oss.html</p> <p>https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html</p> <p>docker run --gpus all \\     -p 8000:8000 \\     --ipc=host \\     vllm/vllm-openai:gptoss \\     --model openai/gpt-oss-20b</p> <p>uv pip install --pre vllm==0.10.1+gptoss \\     --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\     --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\     --index-strategy unsafe-best-match</p> <p>vllm serve openai/gpt-oss-120b</p> <p>https://huggingface.co/blog/welcome-openai-gpt-oss</p> <p>RoPe - https://arxiv.org/abs/2104.09864</p> <p>MoE - https://arxiv.org/abs/1701.06538</p> <p>GPT3- https://arxiv.org/abs/2005.14165</p> <p>GPT 2 -  https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</p> <p>https://cookbook.openai.com/articles/run-nvidia</p> <p>https://developer.nvidia.com/blog/delivering-1-5-m-tps-inference-on-nvidia-gb200-nvl72-nvidia-accelerates-openai-gpt-oss-models-from-cloud-to-edge/</p>"},{"location":"misc/v2/steps-arm64-cpu.html","title":"Steps arm64 cpu","text":"<p>sudo apt-get install liblzma-dev</p> <p>sudo apt-get install libnuma-dev</p> <p>sudo apt update &amp;&amp; sudo apt upgrade -y</p> <p>sudo apt install -y software-properties-common build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev curl libsqlite3-dev wget llvm libbz2-dev tk-dev</p> <p>cd /usr/src sudo wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tgz sudo tar -xf Python-3.12.4.tgz cd Python-3.12.4</p> <p>sudo ./configure --enable-optimizations</p> <p>sudo make -j$(nproc) sudo make altinstall</p> <p>python3.12 --version</p> <p>sudo snap install astral-uv --classic uv pip install torch torchvision python use_existing_torch.py  pip install --upgrade setuptools twine setuptools-scm pip install -r requirements/cpu.txt</p> <p>export MAX_JOBS=4 export VLLM_TARGET_DEVICE=cpu</p> <p>python setup.py bdist_wheel pip install dist/*.whl</p>"},{"location":"misc/v2/tensorRT-llm.html","title":"tensorRT llm","text":"<p>gpt-oss via tensorRT-LLM</p> <p>sudo docker run --rm --ipc=host -it \\   --ulimit stack=67108864 \\   --ulimit memlock=-1 \\   --gpus all \\   -p 8000:8000 \\   -e TRTLLM_ENABLE_PDL=1 \\   -e TRT_LLM_DISABLE_LOAD_WEIGHTS_IN_PARALLEL=True \\   -v ~/.cache:/root/.cache:rw \\   nvcr.io/nvidia/tensorrt-llm/release:gpt-oss-dev \\   /bin/bash</p> <p>cat &lt; low_latency.yaml enable_attention_dp: false enable_mixed_sampler: true cuda_graph_config:     max_batch_size: 8     enable_padding: true moe_config:     backend: TRITON EOF <p>mpirun -n 1 --oversubscribe --allow-run-as-root \\ trtllm-serve  openai/gpt-oss-20b \\   --host 0.0.0.0 \\   --port 9500 \\   --backend pytorch \\   --tp_size 1 \\   --ep_size 1 \\   --trust_remote_code \\   --extra_llm_api_options low_latency.yaml \\   --kv_cache_free_gpu_memory_fraction 0.75</p> <p>curl -s -o /dev/null -w \"Status: %{http_code}\\n\" \"http://localhost:9500/health\"  </p> <p>curl localhost:9500/v1/chat/completions -H \"Content-Type: application/json\" -d '{     \"model\": \"openai/gpt-oss-120b\",     \"messages\": [         {             \"role\": \"user\",             \"content\": \"What is NVIDIAs advantage for inference?\"         }     ],     \"max_tokens\": 1024,     \"top_p\": 0.9 }' -w \"\\n\"</p>"},{"location":"misc/v2/vllm-pd-disaggregation.html","title":"Vllm pd disaggregation","text":"<p>vLLM - PD  Disaggregation</p> <ul> <li>Disaggregared Prefill<ul> <li>https://docs.vllm.ai/en/stable/examples/online_serving/disaggregated_prefill.html</li> </ul> </li> <li> <p>Disaggregated Serving</p> <ul> <li>https://docs.vllm.ai/en/stable/examples/online_serving/disaggregated_serving.html#example-materials</li> </ul> </li> <li> <p>Refrence</p> <ul> <li>https://github.com/vllm-project/vllm/issues/21800</li> </ul> </li> </ul>"},{"location":"misc/v3/deepseek_setup.html","title":"Deepseek setup","text":"<p>on-b200</p> <p>sudo apt update sudo apt upgrade -y</p> <p>sudo apt install python3.12 python3.12-venv python3.12-dev -y</p> <p>python3.12 -m venv venv source venv/bin/activate</p> <p>pip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128</p> <p>pip install vllm</p> <p>vllm serve deepseek-ai/DeepSeek-V3.1 \\   --trust-remote-code \\   --tensor-parallel-size 8 \\   --enable-expert-parallel \\   --max-model-len 154000 \\   --port 8000</p>"},{"location":"misc/v3/ux_setup.html","title":"Ux setup","text":"<p>python3.10 -m venv venv source venv/bin/activate pip install mkdocs-material</p> <p>mkdocs new .</p> <p>mkdocs serve</p> <p>mkdocs build</p>"},{"location":"mobile/index.html","title":"readme","text":"<ul> <li>Android App Setup<ul> <li>Play Store Link</li> </ul> </li> </ul>"},{"location":"mobile/android-app.html","title":"android","text":"<p>dwani.ai - Android App</p> <ul> <li>https://github.com/dwani-ai/dwani-android</li> </ul>"},{"location":"mobile/android-app.html#dwani-ai-android-app","title":"dwani AI - Android App","text":"<p>dwani AI is an innovative Android application designed to provide advanced voice-based AI functionalities, including voice detection, translation, document processing, and interactive answers. The app leverages cutting-edge AI technologies to deliver a seamless and intuitive user experience, making it a versatile tool for communication and productivity.</p>"},{"location":"mobile/android-app.html#release-notes","title":"Release Notes","text":"<ul> <li>Update the dwani Server URL in below files , replace example.com with your dwani API server</li> <li>app/src/main/java/com/slabstech/dwani/voiceai/Api.kt</li> <li>app/src/main/res/xml/preferences.xml</li> </ul>"},{"location":"mobile/android-app.html#features","title":"Features","text":"<ul> <li>Voice Detection: Capture and process audio input for real-time voice interactions.</li> <li>Translation: Translate text or speech across multiple languages.</li> <li>Document Processing: Access and manage documents with AI-powered insights.</li> <li>Interactive Answers: Get intelligent responses to queries through the Answer Activity.</li> <li>Onboarding: Guided setup for new users to explore app features.</li> </ul>"},{"location":"mobile/android-app.html#app-components","title":"App Components","text":"<p>The app consists of the following key activities and services, as defined in the Android manifest:</p> <ul> <li>LoginActivity: Entry point for user authentication.</li> <li>dwaniActivity: Core activity for primary app functionalities.</li> <li>OnboardingActivity: Walkthrough for first-time users.</li> <li>SettingsActivity: Configuration options for user preferences.</li> <li>AnswerActivity: Displays AI-generated answers to user queries.</li> <li>TranslateActivity: Handles translation tasks.</li> <li>DocsActivity: Manages document-related features.</li> <li>VoiceDetectionActivity: Processes voice input for AI interactions.</li> </ul>"},{"location":"mobile/android-app.html#permissions","title":"Permissions","text":"<p>dwani AI requires the following permissions to function effectively:</p> <ul> <li>RECORD_AUDIO: Enables voice input for voice detection and interaction.</li> <li>READ_EXTERNAL_STORAGE (up to SDK 32): Accesses external storage for document processing.</li> <li>READ_MEDIA_IMAGES (up to SDK 33): Reads images from media storage.</li> <li>INTERNET: Facilitates online AI processing, translation, and data retrieval.</li> </ul>"},{"location":"mobile/android-app.html#prerequisites","title":"Prerequisites","text":"<p>To build and run dwani AI, ensure you have the following:</p> <ul> <li>Android Studio: Version 4.0 or higher.</li> <li>JDK: Version 11 or higher.</li> <li>Android SDK: API level 21 (Lollipop) or higher.</li> <li>Gradle: Compatible with the project's build configuration.</li> <li>Device/Emulator: Android 5.0+ for testing.</li> </ul>"},{"location":"mobile/android-app.html#installation","title":"Installation","text":"<ol> <li> <p>Clone the Repository:    <code>bash    git clone https://github.com/slabstech/dwani-android</code></p> <p>Open in Android Studio:     Launch Android Studio.     Select Open an existing project and navigate to the cloned repository folder. Sync Project:     Click Sync Project with Gradle Files to resolve dependencies. Build the App:     Go to Build &gt; Make Project or press Ctrl+F9. Run the App:     Connect an Android device or start an emulator.     Click Run &gt; Run 'app' or press Shift+F10.</p> </li> </ol> <p>Project Structure <pre><code>dwani-ai/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 main/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 java/com/slabstech/dwani/voiceai/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dwaniApp.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 LoginActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dwaniActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 OnboardingActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 SettingsActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 AnswerActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 TranslateActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 DocsActivity.java\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 VoiceDetectionActivity.java\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 res/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 layout/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mipmap/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 values/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 AndroidManifest.xml\n\u2502   \u251c\u2500\u2500 build.gradle\n\u251c\u2500\u2500 gradle/\n\u251c\u2500\u2500 build.gradle\n\u251c\u2500\u2500 settings.gradle\n</code></pre></p> <p>Configuration</p> <pre><code>App Name: Defined in res/values/strings.xml as app_name.\nIcons: Launcher icons are stored in res/mipmap/ (ic_launcher and ic_launcher_round).\nTheme: Uses a custom light theme (Theme.dwaniVoiceAI.Light).\nBackup: Enabled with android:allowBackup=\"true\".\n</code></pre> <p>Dependencies The app uses standard Android libraries and services, including:</p> <pre><code>AndroidX: For modern Android components.\nWorkManager: For background tasks (SystemAlarmService, SystemJobService).\nAdditional dependencies are specified in app/build.gradle.\n</code></pre> <p>Usage</p> <pre><code>Launch the App: Start with LoginActivity to sign in or create an account.\nOnboarding: New users are guided through features via OnboardingActivity.\nExplore Features:\n    Use VoiceDetectionActivity for voice commands.\n    Access TranslateActivity for language translation.\n    Manage documents in DocsActivity.\n    View AI responses in AnswerActivity.\nCustomize: Adjust settings in SettingsActivity.\n</code></pre> <p>Contributing Contributions are welcome! To contribute:</p> <pre><code>Fork the repository.\nCreate a feature branch (git checkout -b feature/your-feature).\nCommit changes (git commit -m 'Add your feature').\nPush to the branch (git push origin feature/your-feature).\nOpen a pull request.\n</code></pre> <p>License This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"mobile/fdroid.html","title":"Fdroid","text":"<p>fdroid Setup</p> <p>https://f-droid.org/docs/Submitting_to_F-Droid_Quick_Start_Guide/</p>"},{"location":"multimodal/index.html","title":"readme","text":"<ul> <li>ASR for Indian languages Setup</li> <li>TTS for Indian languages Setup</li> <li>Translate Server Setup</li> <li>ASR +TTS for English Setup</li> <li>Document API server Setup</li> </ul>"},{"location":"multimodal/asr_server.html","title":"ASR Server","text":""},{"location":"multimodal/asr_server.html#asr-automatic-speech-recognition-speech-to-text","title":"ASR - Automatic Speech Recognition / Speech to Text","text":"<ul> <li>https://github.com/dwani-ai/asr-indic-server.git <pre><code>git clone https://github.com/dwani-ai/asr-indic-server.git\ncd asr-indic-server\n\npython -m venv --system-site-packages venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n\n\npython src/multilingual/asr_api.py --host 0.0.0.0 --port 7863 --device cuda\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#asr-indic-server","title":"ASR Indic Server","text":""},{"location":"multimodal/asr_server.html#overview","title":"Overview","text":"<p>Automatic Speech Recognition (ASR) for Indian languages using IndicConformer models. The default model is set to Kannada ASR.</p>"},{"location":"multimodal/asr_server.html#demo-video","title":"Demo Video","text":"<p>Watch a quick demo of our project in action! Click the image below to view the video on YouTube.</p> <p> </p>"},{"location":"multimodal/asr_server.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Supported Languages</li> <li>Live Server</li> <li>Getting Started</li> <li>For Production (Docker)<ul> <li>Prerequisites</li> <li>Steps</li> </ul> </li> <li>For Development (Local)<ul> <li>Prerequisites</li> <li>Steps</li> </ul> </li> <li>Downloading Translation Models</li> <li>Kannada</li> <li>Other Languages<ul> <li>Malayalam</li> <li>Hindi</li> </ul> </li> <li>Running with FastAPI Server</li> <li>Live Server</li> <li>Service Modes<ul> <li>High Latency, Slow System (Available 24/7)</li> <li>Low Latency, Fast System (Available on Request)</li> </ul> </li> <li>How to Use the Service<ul> <li>High Latency Service</li> <li>Low Latency Service</li> <li>Notes</li> </ul> </li> <li>Evaluating Results</li> <li>Kannada Transcription Examples<ul> <li>Sample 1: kannada_sample_1.wav</li> <li>Sample 2: kannada_sample_2.wav</li> <li>Sample 3 - Song - 4 minutes</li> <li>Sample 4 - Song - 6.4 minutes</li> </ul> </li> <li>Batch Transcription Examples<ul> <li>Transcribe Batch Endpoint</li> </ul> </li> <li>Building Docker Image</li> <li>Run the Docker Image</li> <li>Troubleshooting</li> <li>References</li> <li>Additional Resources</li> <li>Running Nemo Model</li> <li>Running with Transformers</li> </ul>"},{"location":"multimodal/asr_server.html#supported-languages","title":"Supported Languages","text":"<p>22 Indian languages are supported, thanks to AIBharat organisation</p> Language Code Assamese <code>as</code> Bengali <code>bn</code> Bodo <code>brx</code> Dogri <code>doi</code> Gujarati <code>gu</code> Hindi <code>hi</code> Kannada <code>kn</code> Kashmiri <code>ks</code> Konkani <code>kok</code> Maithili <code>mai</code> Malayalam <code>ml</code> Manipuri <code>mni</code> Marathi <code>mr</code> Nepali <code>ne</code> Odia <code>or</code> Punjabi <code>pa</code> Sanskrit <code>sa</code> Santali <code>sat</code> Sindhi <code>sd</code> Tamil <code>ta</code> Telugu <code>te</code> Urdu <code>ur</code>"},{"location":"multimodal/asr_server.html#live-server","title":"Live Server","text":"<p>We have hosted an Automatic Speech Recognition (ASR) service that can be used to verify the accuracy of audio transcriptions. </p>"},{"location":"multimodal/asr_server.html#_1","title":"ASR Server","text":"<ul> <li> <p>CPU - API Endpoint</p> </li> <li> <p>Via Gradio UI</p> </li> </ul>"},{"location":"multimodal/asr_server.html#notes","title":"Notes","text":"<ul> <li>Ensure that the audio file path (<code>samples/kannada_sample_2.wav</code>) is correct and accessible.</li> <li>The <code>language</code> parameter in the URL specifies the language of the audio file. In the examples above, it is set to <code>kannada</code>.</li> <li>The service expects the audio file to be in WAV format.</li> </ul>"},{"location":"multimodal/asr_server.html#getting-started-development","title":"Getting Started - Development","text":""},{"location":"multimodal/asr_server.html#for-development-local","title":"For Development (Local)","text":"<ul> <li>Prerequisites: Python 3.10 (compatibility verified)</li> <li>Steps:</li> <li>Create a virtual environment:   <pre><code>python -m venv venv\n</code></pre></li> <li>Activate the virtual environment:   <pre><code>source venv/bin/activate\n</code></pre>   On Windows, use:   <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>Install dependencies:       <pre><code>pip install -r requirements.txt\n</code></pre><ul> <li> </li> </ul> </li> </ul>"},{"location":"multimodal/asr_server.html#for-individual-language-models","title":"For Individual language models","text":"<pre><code>pip install -r nemo-requirements.txt\n</code></pre>"},{"location":"multimodal/asr_server.html#downloading-translation-models","title":"Downloading Translation Models","text":"<p>Models can be downloaded from AI4Bharat's HuggingFace repository:</p>"},{"location":"multimodal/asr_server.html#for-multi-lingual-language-supported-model","title":"For Multi-lingual language supported model","text":"<pre><code>huggingface-cli download ai4bharat/indic-conformer-600m-multilingual\n</code></pre>"},{"location":"multimodal/asr_server.html#for-individual-langauge-models","title":"For Individual langauge models","text":"<ul> <li>Kannada   <pre><code>huggingface-cli download ai4bharat/indicconformer_stt_kn_hybrid_rnnt_large\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#other-languages","title":"Other Languages","text":"<ul> <li> <p>ASR  - IndicConformer Collection on HuggingFace</p> </li> <li> <p>Malayalam   <pre><code>huggingface-cli download ai4bharat/indicconformer_stt_ml_hybrid_rnnt_large\n</code></pre></p> </li> <li> <p>Hindi   <pre><code>huggingface-cli download ai4bharat/indicconformer_stt_hi_hybrid_rnnt_large\n</code></pre></p> </li> </ul>"},{"location":"multimodal/asr_server.html#sample-code","title":"Sample Code","text":""},{"location":"multimodal/asr_server.html#for-all-languages","title":"For all languages","text":"<pre><code>from transformers import AutoModel\nimport torchaudio\nimport torch\n\n# Load the model\nmodel = AutoModel.from_pretrained(\"ai4bharat/indic-conformer-600m-multilingual\", trust_remote_code=True)\n\n# Load an audio file\nwav, sr = torchaudio.load(\"kannada_sample_1.wav\")\nwav = torch.mean(wav, dim=0, keepdim=True)\n\ntarget_sample_rate = 16000  # Expected sample rate\nif sr != target_sample_rate:\n    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sample_rate)\n    wav = resampler(wav)\n\n# Perform ASR with CTC decoding\ntranscription_ctc = model(wav, \"kn\", \"ctc\")\nprint(\"CTC Transcription:\", transcription_ctc)\n\n# Perform ASR with RNNT decoding\ntranscription_rnnt = model(wav, \"kn\", \"rnnt\")\nprint(\"RNNT Transcription:\", transcription_rnnt)\n</code></pre> <ul> <li>Run the Code   <pre><code>python asr-multi-lingual.py\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#individual-languages","title":"Individual Languages","text":"<pre><code>import torch\nimport nemo.collections.asr as nemo_asr\n\nmodel = nemo_asr.models.ASRModel.from_pretrained(\"ai4bharat/indicconformer_stt_kn_hybrid_rnnt_large\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.freeze() # inference mode\nmodel = model.to(device)\n\nmodel.cur_decoder = \"rnnt\"\nrnnt_text = model.transcribe(['samples/kannada_sample_1.wav'], batch_size=1, language_id='kn')[0]\n\n\nprint(rnnt_text)\n</code></pre> <ul> <li>Run the Code   <pre><code>python asr_code.py\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#alternative-examples-for-development","title":"Alternative examples for Development","text":""},{"location":"multimodal/asr_server.html#for-local-development","title":"For Local Development","text":"<ul> <li>Gradio  <pre><code>python src/ux/app_local.py\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#for-server-development","title":"For Server Development","text":""},{"location":"multimodal/asr_server.html#running-with-fastapi-server","title":"Running with FastAPI Server","text":"<p>Run the server using FastAPI with the desired language (e.g., Kannada): - for GPU   <pre><code>python src/multi-lingual/asr_api.py --port 7860 --language kn --host 0.0.0.0 --device gpu\n</code></pre> - for CPU only   <pre><code>python src/multi-lingual/asr_api.py --port 7860 --language kn --host 0.0.0.0 --device cpu\n</code></pre></p>"},{"location":"multimodal/asr_server.html#evaluating-results-for-fastapi-server","title":"Evaluating Results for FastApi Server","text":"<p>You can evaluate the ASR transcription results using <code>curl</code> commands. </p>"},{"location":"multimodal/asr_server.html#kannada-transcription-examples","title":"Kannada Transcription Examples","text":""},{"location":"multimodal/asr_server.html#sample-1-kannada_sample_1wav","title":"Sample 1: kannada_sample_1.wav","text":"<ul> <li>Audio File: samples/kannada_sample_1.wav</li> <li>Command: <pre><code>curl -X 'POST' 'http://loca?language=kannada' -H 'accept: application/json'   -H 'Content-Type: multipa'Content-Type  multipart/form-data' -F 'file=@samples/kannada_sample_1.wav;type=audio/x-wav'\n</code></pre></li> <li>Expected Output: <code>\u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0caf\u0cbe\u0cb5\u0cc1\u0ca6\u0cc1</code> Translation: \"What is the capital of Karnataka\"</li> </ul>"},{"location":"multimodal/asr_server.html#sample-2-kannada_sample_2wav","title":"Sample 2: kannada_sample_2.wav","text":"<ul> <li>Audio File: samples/kannada_sample_2.wav</li> <li>Command: <pre><code>curl -X 'POST' \\\n'http://localhost:7860/transcribe/?language=kannada' \\\n-H 'accept: application/json'   -H 'Content-Type: multipart/form-data' \\\n-F 'file=@samples/kannada_sample_2.wav;type=audio/x-wav'\n</code></pre></li> <li>Expected Output: <code>\u0cac\u0cc6\u0c82\u0c97\u0cb3\u0cc2\u0cb0\u0cc1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0c86\u0c97\u0cbf\u0ca6\u0cc6 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cbe\u0cb5\u0cc1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cc7\u0cb5\u0cc6</code></li> </ul>"},{"location":"multimodal/asr_server.html#sample-3-song-4-minutes","title":"Sample 3 - Song - 4 minutes","text":"<ul> <li>YT Video- Navaduva Nudiye</li> <li>Audio File: samples/kannada_sample_3.wav</li> <li>Command: <pre><code>curl -X 'POST' \\\n'http://localhost:7860/transcribe/language=kannada' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'file=@samples/kannada_sample_3.wav;type=audio/x-wav'\n</code></pre></li> <li>Expected Output: kannada_sample_3_out.md</li> </ul>"},{"location":"multimodal/asr_server.html#sample-4-song-64-minutes","title":"Sample 4 - Song - 6.4 minutes","text":"<ul> <li>YT Video- Aagadu Yendu</li> <li>Audio File: samples/kannada_sample_4.wav</li> <li>Command: <pre><code>curl -X 'POST' \\\n'http://localhost:7860/transcribe/language=kannada' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'file=@samples/kannada_sample_4.wav;type=audio/x-wav'\n</code></pre></li> <li>Expected Output: kannada_sample_4_out.md</li> </ul> <p>Note: The ASR does not provide sentence breaks or punctuation (e.g., question marks). We plan to integrate an LLM parser for improved context in future updates.</p>"},{"location":"multimodal/asr_server.html#batch-transcription-examples","title":"Batch Transcription Examples","text":""},{"location":"multimodal/asr_server.html#transcribe-batch-endpoint","title":"Transcribe Batch Endpoint","text":"<p>The <code>/transcribe_batch</code> endpoint allows you to transcribe multiple audio files in a single request. This is useful for batch processing of audio files.</p> <ul> <li>Command: <pre><code>curl -X 'POST' \\\n'http://localhost:7860/transcribe_batch/' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'files=@samples/kannada_sample_1.wav;type=audio/x-wav' \\\n-F 'files=@samples/kannada_sample_2.wav;type=audio/x-wav'\n</code></pre></li> <li>Expected Output: <pre><code>{\n  \"transcriptions\": [\n    \"\u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0caf\u0cbe\u0cb5\u0cc1\u0ca6\u0cc1\",\n    \"\u0cac\u0cc6\u0c82\u0c97\u0cb3\u0cc2\u0cb0\u0cc1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0c86\u0c97\u0cbf\u0ca6\u0cc6 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cbe\u0cb5\u0cc1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cc7\u0cb5\u0cc6\"\n  ]\n}\n</code></pre></li> </ul>"},{"location":"multimodal/asr_server.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Transcription errors: Verify the audio file is in WAV format, mono, and sampled at 16kHz. Adjust using: <pre><code>ffmpeg -i sample_audio.wav -ac 1 -ar 16000 sample_audio_infer_ready.wav -y\n</code></pre></li> <li>Model not found: Download the required models using the <code>huggingface-cli download</code> commands above.</li> <li>Port conflicts: Ensure port 7860 is free when running the FastAPI server.</li> </ul>"},{"location":"multimodal/asr_server.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read the CONTRIBUTING.md file for guidelines on how to contribute to this project.</p> <p>Also you can join the discord group to collaborate</p>"},{"location":"multimodal/asr_server.html#references","title":"References","text":"<ul> <li>AI4Bharat IndicConformerASR GitHub Repository</li> <li>Nemo - AI4Bharat</li> <li>IndicConformer Collection on HuggingFace</li> </ul>"},{"location":"multimodal/asr_server.html#additional-methods-for-development","title":"Additional methods for Development","text":""},{"location":"multimodal/asr_server.html#running-nemo-model","title":"Running Nemo Model","text":"<ol> <li>Download the Nemo model: <pre><code>wget https://objectstore.e2enetworks.net/indic-asr-public/indicConformer/ai4b_indicConformer_kn.nemo -O kannada.nemo\n</code></pre></li> <li>Adjust the audio: <pre><code>ffmpeg -i sample_audio.wav -ac 1 -ar 16000 sample_audio_infer_ready.wav -y\n</code></pre></li> <li>Run the program: <pre><code>python nemo_asr.py\n</code></pre></li> </ol>"},{"location":"multimodal/asr_tts_english.html","title":"Asr tts english","text":"<p>ASR - English</p> <p>curl --silent --remote-name https://raw.githubusercontent.com/speaches-ai/speaches/master/compose.yaml curl --silent --remote-name https://raw.githubusercontent.com/speaches-ai/speaches/master/compose.cuda.yaml export COMPOSE_FILE=compose.cuda.yaml</p> <p>sudo docker compose -f compose.cuda.yaml up -d</p> <p>curl -X 'POST' \\   'http://localhost:8000/v1/models/Systran/faster-whisper-small' \\   -H 'accept: application/json' \\   -d ''</p> <p>curl -X 'POST' \\   'http://localhost:8000/v1/models/speaches-ai/Kokoro-82M-v1.0-ONNX' \\   -H 'accept: application/json' \\   -d ''</p>"},{"location":"multimodal/docs_setup.html","title":"Docs API","text":""},{"location":"multimodal/docs_setup.html#docs-indic-server","title":"docs-indic-server","text":"<pre><code>git clone https://github.com/dwani-ai/docs-indic-server.git\ncd docs-indic-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\n\npip install -r requirements.txt\npip install \"numpy&lt;2.0\"\n\npython src/server/docs_api.py  --host 0.0.0.0 --port 7861\n</code></pre> <ul> <li>Dependencies</li> <li>decord <pre><code>cd\nmkdir external\ncd external\ngit clone --recursive https://github.com/dmlc/decord\n\ncd decord\n\nmkdir build &amp;&amp; cd build\n\nexport CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda\nexport CUDACXX=/usr/local/cuda/bin/nvcc\n\nnvcc --version\n\ncmake .. -DUSE_CUDA=1 -DCMAKE_BUILD_TYPE=Release -DFFMPEG_DIR=/usr\n\ncmake .. -DUSE_CUDA=1 -DCMAKE_BUILD_TYPE=Release\n\nmake\n\ncd ../python\npython3 setup.py install --user\npip install \"numpy&lt;2.0\"\n</code></pre></li> <li>olmocr <pre><code>cd ../../\ngit clone  https://github.com/allenai/olmocr.git\n\ncd olmocr\n\npip install --upgrade pip setuptools wheel packaging\n\n // copy ppyproject.toml\npip install -e .\ncd ../../dwani_org/gh-200-docs-indic-server/\npip install \"numpy&lt;2.0\"\n</code></pre></li> <li>ffmpeg   <pre><code>sudo apt update\nsudo apt install ffmpeg\nffmpeg -version\n\nsudo apt update\nsudo apt install cmake ffmpeg build-essential python3-dev\n\nsudo apt install pkg-config libavcodec-dev libavformat-dev libavutil-dev libswscale-dev libswresample-dev  libavfilter-dev \n\nsudo apt-get install poppler-utils\n</code></pre></li> </ul>"},{"location":"multimodal/docs_setup.html#docs-indic-server_1","title":"Docs-Indic-Server","text":""},{"location":"multimodal/docs_setup.html#overview","title":"Overview","text":"<p>Document parser for Indian languages</p>"},{"location":"multimodal/docs_setup.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Features</li> <li>Getting Started - Development</li> <li>Downloading Model</li> <li>Running with FastAPI Server</li> <li>Evaluating Results</li> <li>Citations</li> </ul>"},{"location":"multimodal/docs_setup.html#features","title":"Features","text":"<ul> <li>Extract text from PDF - Single Page, Multiple, Full</li> <li>Extract text from Image</li> <li>Summary text from Image/PDF<ul> <li>English</li> <li>Kannada</li> <li>German</li> </ul> </li> <li>Recreate PDF -&gt; Scanned doc to clean PDF</li> <li>Convert PDF -&gt;<ul> <li>English to Kannada</li> <li>Kannada to English</li> </ul> </li> </ul>"},{"location":"multimodal/docs_setup.html#for-development","title":"For Development","text":"<ul> <li>Prerequisites: Python 3.6+</li> <li>Steps:</li> <li>Create a virtual environment:   <pre><code>python -m venv venv\n</code></pre></li> <li>Activate the virtual environment:   <pre><code>source venv/bin/activate\n</code></pre>   On Windows, use:   <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>Install dependencies:</li> <li> <p><code>bash     pip install -r requirements.txt</code></p> </li> <li> <p>Backend Server  - Select based on GPU VRAM</p> </li> <li><code>bash     vllm serve google/gemma-3-4b-it</code></li> <li><code>bash     vllm serve reducto/RolmOCR</code></li> <li> <p><code>bash     vllm serve google/gemma-3-12b-it</code></p> </li> <li> <p>for H100 only</p> </li> <li> <p>google/gemma-3-12b-it</p> </li> <li> <p>for A100 only</p> </li> <li>google/gemma-3-12b-it</li> </ul>"},{"location":"multimodal/docs_setup.html#running-with-fastapi-server","title":"Running with FastAPI Server","text":"<ul> <li><pre><code>python src/server/docs_api_dwani.py --port 7860 --host 0.0.0.0\n</code></pre></li> </ul>"},{"location":"multimodal/docs_setup.html#gpu-server-setup","title":"GPU server setup","text":"<ul> <li>Terminal 1      <pre><code>git clone https://github.com/slabstech/docs-indic-server.git\ncd docs-indic-server\nchmod +x install-script.sh\nbash install-script.sh\nexport HF_TOKEN='YOUR-HF-TOKEN'\nexport HF_HOME=/home/ubuntu/data-dhwani-models\nvllm serve google/gemma-3-4b-it\n</code></pre></li> <li>Terminal 2     <pre><code>cd docs-indic-server\nsource venv/bin/activate\nexport HF_TOKEN='YOUR-HF-TOKEN'\nexport HF_HOME=/home/ubuntu/data-dhwani-models\npython src/server/docs_api.py --port 7860 --host 0.0.0.0\n</code></pre></li> <li>Terminal 3     <pre><code>git clone https://github.com/slabstech/indic-translate-server\ncd indic-translate-server\npython3.10 -m venv venv\nsource venv/bin/activate\npip install -r server-requirements.txt\nexport HF_TOKEN='YOUR-HF-TOKEN'\nexport HF_HOME=/home/ubuntu/data-dhwani-models\nhuggingface-cli download ai4bharat/indictrans2-indic-en-dist-200M\nhuggingface-cli download ai4bharat/indictrans2-en-indic-dist-200M\npython src/server/translate_api.py --port 7861 --host 0.0.0.0 --device cuda --use_distilled\n</code></pre></li> </ul> <p>--  For  kannad pdf  we need to add - NotoSans Kannada font from google</p> <p>https://fonts.google.com/noto/specimen/Noto+Sans+Kannada</p>"},{"location":"multimodal/docs_setup.html#httpsgithubcomgooglefontsnoto-fonts","title":"https://github.com/googlefonts/noto-fonts","text":""},{"location":"multimodal/docs_setup.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read the CONTRIBUTING.md file for guidelines on how to contribute to this project.</p> <p>Also you can join the discord group to collaborate</p> <ul> <li>Reference<ul> <li>HF - moondream</li> <li>source - moondream</li> <li>moondream-blog</li> <li>pixtral-12-b-2409</li> </ul> </li> </ul>"},{"location":"multimodal/docs_setup.html#citations","title":"Citations","text":"<p>```bibtex citation_1.bib @misc{poznanski2025olmocrunlockingtrillionstokens,       title={olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models},        author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},       year={2025},       eprint={2502.18443},       archivePrefix={arXiv},       primaryClass={cs.CL},       url={https://arxiv.org/abs/2502.18443},  } <pre><code>&lt;!-- \n\n\n\n\nwget https://github.com/slabstech/docs-indic-server/blob/01e811210d56e655091313c1df8481d11e7640a6/install-script.sh\nchmod +x install-script.sh\nbash install-script.sh\n\n\n\n## Download Qwen VL\n\n```bash download_model.sh\nhuggingface_cli download google/gemma-3-4b-it\n</code></pre></p>"},{"location":"multimodal/docs_setup.html#download-gemma","title":"Download Gemma","text":"<p>```bash download_model.sh huggingface_cli download google/gemma-3-4b-it <pre><code>## Download Pixtral \n\n```bash download_model.sh\nhuggingface_cli download mistralai/Pixtral-12B-2409\n</code></pre></p>"},{"location":"multimodal/docs_setup.html#download-moondream2","title":"Download Moondream2","text":"<p><pre><code>huggingface_cli  vikhyatk/moondream2\n</code></pre> Model Size - 4GB</p>"},{"location":"multimodal/docs_setup.html#getting-started-development","title":"Getting Started - Development","text":"<ul> <li>For moondream, libvips system library is required    <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install libvips\n</code></pre></li> </ul>"},{"location":"multimodal/docs_setup.html#evaluating-results","title":"Evaluating Results","text":"<p>You can evaluate the ASR transcription results using <code>curl</code> commands. Below are examples for Kannada audio samples.</p>"},{"location":"multimodal/docs_setup.html#kannada","title":"Kannada","text":"<p>```bash kannada_example.sh curl -s -H \"content-type: application/json\" localhost:7860/v1/audio/speech -d '{\"input\": \"\u0c89\u0ca6\u0ccd\u0caf\u0cbe\u0ca8\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0cae\u0c95\u0ccd\u0c95\u0cb3 \u0c86\u0c9f\u0cb5\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cbf\u0ca6\u0ccd\u0ca6\u0cbe\u0cb0\u0cc6 \u0cae\u0ca4\u0ccd\u0ca4\u0cc1 \u0caa\u0c95\u0ccd\u0cb7\u0cbf\u0c97\u0cb3\u0cc1 \u0c9a\u0cbf\u0cb2\u0cbf\u0caa\u0cbf\u0cb2\u0cbf \u0cae\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cbf\u0cb5\u0cc6.\"}' -o audio_kannada.mp3 <pre><code>#### Hindi\n\n```bash hindi_example.sh\ncurl -s -H \"content-type: application/json\" localhost:7860/v1/audio/speech -d '{\"input\": \"\u0905\u0930\u0947, \u0924\u0941\u092e \u0906\u091c \u0915\u0948\u0938\u0947 \u0939\u094b?\"}' -o audio_hindi.mp3\n</code></pre></p>"},{"location":"multimodal/docs_setup.html#specifying-a-different-format","title":"Specifying a Different Format","text":"<p>```bash specify_format.sh curl -s -H \"content-type: application/json\" localhost:7860/v1/audio/speech -d '{\"input\": \"Hey, how are you?\", \"response_type\": \"wav\"}' -o audio.wav <pre><code>### For Production (Docker)\n- **Prerequisites**: Docker and Docker Compose\n- **Steps**:\n  1. **Start the server**:\n  For GPU\n  ```bash\n  docker compose -f compose.yaml up -d\n  ```\n  For CPU only\n  ```bash\n  docker compose -f cpu-compose.yaml up -d\n  ```\n\n#  - vllm serve vikhyatk/moondream2 --trust-remote-code\n\n\n## Building Docker Image\nBuild the Docker image locally:\n```bash\ndocker build -t slabstech/docs_indic_server -f Dockerfile .\n</code></pre></p>"},{"location":"multimodal/docs_setup.html#run-the-docker-image","title":"Run the Docker Image","text":"<pre><code>docker run --gpus all -it --rm -p 7860:7860 slabstech/docs_indic_server\n</code></pre> <p>--&gt;</p>"},{"location":"multimodal/translate_server.html","title":"Translate Server","text":"<p>Translate Server</p> <ul> <li>https://github.com/dwani-ai/indic-translate-server.git <pre><code>git clone https://github.com/dwani-ai/indic-translate-server.git\ncd indic-translate-server\n\npython -m venv --system-site-packages venv\n\nsource venv/bin/activate\npip install --upgrade pip setuptools wheel packaging cython\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\n#pip install -r requirements.txt\n\npip install -e git+https://github.com/VarunGumma/IndicTransToolkit.git@main#egg=IndicTransToolkit\n\npip install fastapi uvicorn  \"numpy&lt;2.0\"\npython src/server/translate_api.py --host 0.0.0.0 --port 7862 --device cpu\n</code></pre></li> </ul>"},{"location":"multimodal/translate_server.html#indic-translate-server","title":"Indic Translate Server","text":""},{"location":"multimodal/translate_server.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Live Server</li> <li>Prerequisites</li> <li>Setting Up the Development Environment</li> <li>Downloading Translation Models</li> <li>Running with FastAPI Server</li> <li>Evaluating Results</li> <li>Build Docker Image</li> <li>References</li> <li>Contributing</li> <li>License</li> <li>FAQ</li> </ul>"},{"location":"multimodal/translate_server.html#overview","title":"Overview","text":"<p>This project sets up an Indic translation server, allowing translation between various languages including English, Kannada, Hindi, and others. It utilizes models from AI4Bharat to perform translations.</p> <p>We suggest to use non-distilled models for better translation. </p>"},{"location":"multimodal/translate_server.html#languages-supported","title":"Languages Supported","text":"<p>Here is the list of languages supported by the IndicTrans2 models:</p> Assamese (asm_Beng) Kashmiri (Arabic) (kas_Arab) Punjabi (pan_Guru) Bengali (ben_Beng) Kashmiri (Devanagari) (kas_Deva) Sanskrit (san_Deva) Bodo (brx_Deva) Maithili (mai_Deva) Santali (sat_Olck) Dogri (doi_Deva) Malayalam (mal_Mlym) Sindhi (Arabic) (snd_Arab) English (eng_Latn) Marathi (mar_Deva) Sindhi (Devanagari) (snd_Deva) Konkani (gom_Deva) Manipuri (Bengali) (mni_Beng) Tamil (tam_Taml) Gujarati (guj_Gujr) Manipuri (Meitei) (mni_Mtei) Telugu (tel_Telu) Hindi (hin_Deva) Nepali (npi_Deva) Urdu (urd_Arab) Kannada (kan_Knda) Odia (ory_Orya)"},{"location":"multimodal/translate_server.html#live-server","title":"Live Server","text":"<p>We have hosted an Translation service for Indian languages. </p>"},{"location":"multimodal/translate_server.html#_1","title":"Translate Server","text":"<ul> <li>https://demo.dwani.ai</li> </ul>"},{"location":"multimodal/translate_server.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10  + VsCode</li> <li>Ubuntu 22.04 </li> <li>Internet access to download translation models.</li> </ul>"},{"location":"multimodal/translate_server.html#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<ol> <li> <p>Create a virtual environment: <pre><code>python -m venv venv\n</code></pre></p> </li> <li> <p>Activate the virtual environment:</p> </li> <li>For Mac/Linux     <pre><code>source venv/bin/activate\n</code></pre></li> <li> <p>On Windows, use:     <pre><code>venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    ```</p> </li> </ol> <p>pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128    pip install -r requirements.txt    ```</p>"},{"location":"multimodal/translate_server.html#model-downloads-for-translation","title":"Model Downloads for Translation","text":"<ul> <li>Collection Models on HuggingFace - IndicTrans2</li> </ul> <p>Below is a table summarizing the available models for different translation tasks:</p> Task Variant Model Name VRAM Size Download Command Indic to English 200M (distilled) indictrans2-indic-en-dist-200M 950 MB <code>huggingface-cli download ai4bharat/indictrans2-indic-en-dist-200M</code> 1B (base) indictrans2-indic-en-1B 4.5 GB <code>huggingface-cli download ai4bharat/indictrans2-indic-en-1B</code> English to Indic 200M (distilled) indictrans2-en-indic-dist-200M 950 MB <code>huggingface-cli download ai4bharat/indictrans2-en-indic-dist-200M</code> 1B (base) indictrans2-en-indic-1B 4.5 GB <code>huggingface-cli download ai4bharat/indictrans2-en-indic-1B</code> Indic to Indic 320M (distilled) indictrans2-indic-indic-dist-320M 950 MB <code>huggingface-cli download ai4bharat/indictrans2-indic-indic-dist-320M</code> 1B (base) indictrans2-indic-indic-1B 4.5 GB <code>huggingface-cli download ai4bharat/indictrans2-indic-indic-1B</code>"},{"location":"multimodal/translate_server.html#sample-code","title":"Sample Code","text":"<pre><code>import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit import IndicProcessor\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nsrc_lang, tgt_lang = \"hin_Deva\", \"eng_Latn\"\nmodel_name = \"ai4bharat/indictrans2-indic-en-dist-200M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name, \n    trust_remote_code=True, \n    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n    attn_implementation=\"flash_attention_2\"\n).to(DEVICE)\n\nip = IndicProcessor(inference=True)\n\ninput_sentences = [\n    \"\u091c\u092c \u092e\u0948\u0902 \u091b\u094b\u091f\u093e \u0925\u093e, \u092e\u0948\u0902 \u0939\u0930 \u0930\u094b\u091c\u093c \u092a\u093e\u0930\u094d\u0915 \u091c\u093e\u0924\u093e \u0925\u093e\u0964\",\n    \"\u0939\u092e\u0928\u0947 \u092a\u093f\u091b\u0932\u0947 \u0938\u092a\u094d\u0924\u093e\u0939 \u090f\u0915 \u0928\u0908 \u092b\u093f\u0932\u094d\u092e \u0926\u0947\u0916\u0940 \u091c\u094b \u0915\u093f \u092c\u0939\u0941\u0924 \u092a\u094d\u0930\u0947\u0930\u0923\u093e\u0926\u093e\u092f\u0915 \u0925\u0940\u0964\",\n]\n\nbatch = ip.preprocess_batch(\n    input_sentences,\n    src_lang=src_lang,\n    tgt_lang=tgt_lang,\n)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Tokenize the sentences and generate input encodings\ninputs = tokenizer(\n    batch,\n    truncation=True,\n    padding=\"longest\",\n    return_tensors=\"pt\",\n    return_attention_mask=True,\n).to(DEVICE)\n\n# Generate translations using the model\nwith torch.no_grad():\n    generated_tokens = model.generate(\n        **inputs,\n        use_cache=True,\n        min_length=0,\n        max_length=256,\n        num_beams=5,\n        num_return_sequences=1,\n    )\n\n# Decode the generated tokens into text\nwith tokenizer.as_target_tokenizer():\n    generated_tokens = tokenizer.batch_decode(\n        generated_tokens.detach().cpu().tolist(),\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True,\n    )\n\n# Postprocess the translations, including entity replacement\ntranslations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\nfor input_sentence, translation in zip(input_sentences, translations):\n    print(f\"{src_lang}: {input_sentence}\")\n    print(f\"{tgt_lang}: {translation}\")\n</code></pre>"},{"location":"multimodal/translate_server.html#run-the-sample-code","title":"Run the sample code","text":"<pre><code>python translate_code.py\n</code></pre>"},{"location":"multimodal/translate_server.html#alternate-forms-of-development","title":"Alternate forms of Development","text":""},{"location":"multimodal/translate_server.html#running-with-fastapi-server","title":"Running with FastAPI Server","text":"<p>Install dependencies: <pre><code>pip install -r server-requirements.txt\n</code></pre></p> <p>You can run the server using FastAPI: 1. with GPU  <pre><code>python src/server/translate_api.py --port 7860 --host 0.0.0.0 --device cuda --use_distilled False\n</code></pre></p> <ol> <li>with CPU only <pre><code>python src/server/translate_api.py --port 7860 --host 0.0.0.0 --device cpu --use_distilled False\n</code></pre></li> </ol>"},{"location":"multimodal/translate_server.html#evaluating-results-for-fastapi-server","title":"Evaluating Results for FastAPI Server","text":"<p>You can evaluate the translation results using <code>curl</code> commands. Here are some examples:</p>"},{"location":"multimodal/translate_server.html#english-to-kannada","title":"English to Kannada","text":"<pre><code>curl -X 'POST' \\\n  'http://localhost:7860/translate?tgt_lang=kan_Knda&amp;src_lang=eng_Latn&amp;device_type=cuda' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sentences\": [\n    \"Hello, how are you?\", \"Good morning!\"\n  ],\n  \"src_lang\": \"eng_Latn\",\n  \"tgt_lang\": \"kan_Knda\"\n}'\n</code></pre> <p>Response: <pre><code>{\n  \"translations\": [\n    \"\u0cb9\u0cb2\u0ccb, \u0cb9\u0cc7\u0c97\u0cbf\u0ca6\u0ccd\u0ca6\u0cc0\u0cb0\u0cbf? \",\n    \"\u0cb6\u0cc1\u0cad\u0ccb\u0ca6\u0caf! \"\n  ]\n}\n</code></pre></p>"},{"location":"multimodal/translate_server.html#kannada-to-english","title":"Kannada to English","text":"<pre><code>curl -X 'POST' \\\n  'http://localhost:7860/translate?src_lang=kan_Knda&amp;tgt_lang=eng_Latn&amp;device_type=cuda' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sentences\": [\n    \"\u0ca8\u0cae\u0cb8\u0ccd\u0c95\u0cbe\u0cb0, \u0cb9\u0cc7\u0c97\u0cbf\u0ca6\u0ccd\u0ca6\u0cc0\u0cb0\u0cbe?\", \"\u0cb6\u0cc1\u0cad\u0ccb\u0ca6\u0caf!\"\n  ],\n  \"src_lang\": \"kan_Knda\",\n  \"tgt_lang\": \"eng_Latn\"\n}'\n</code></pre> <p>Response: <pre><code>{\n  \"translations\": [\"Hello, how are you?\", \"Good morning!\"]\n}\n</code></pre></p>"},{"location":"multimodal/translate_server.html#kannada-to-hindi","title":"Kannada to Hindi","text":"<pre><code>curl -X 'POST' \\\n  'http://localhost:7860/translate?src_lang=kan_Knda&amp;tgt_lang=hin_Deva' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sentences\": [\n    \"\u0ca8\u0cae\u0cb8\u0ccd\u0c95\u0cbe\u0cb0, \u0cb9\u0cc7\u0c97\u0cbf\u0ca6\u0ccd\u0ca6\u0cc0\u0cb0\u0cbe?\", \"\u0cb6\u0cc1\u0cad\u0ccb\u0ca6\u0caf!\"\n  ],\n  \"src_lang\": \"kan_Knda\",\n  \"tgt_lang\": \"hin_Deva\"\n}'\n</code></pre>"},{"location":"multimodal/translate_server.html#response","title":"Response","text":"<p>{   \"translations\": [     \"\u0939\u0948\u0932\u094b, \u0915\u0948\u0938\u093e \u0932\u0917 \u0930\u0939\u093e \u0939\u0948? \",     \"\u0917\u0941\u0921 \u092e\u0949\u0930\u094d\u0928\u093f\u0902\u0917! \"   ] }</p>"},{"location":"multimodal/translate_server.html#cpu","title":"CPU","text":"<pre><code>curl -X 'POST' \\\n  'http://localhost:7860/translate?src_lang=kan_Knda&amp;tgt_lang=eng_Latn&amp;device_type=cpu' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"sentences\": [\n    \"\u0ca8\u0cae\u0cb8\u0ccd\u0c95\u0cbe\u0cb0, \u0cb9\u0cc7\u0c97\u0cbf\u0ca6\u0ccd\u0ca6\u0cc0\u0cb0\u0cbe?\", \"\u0cb6\u0cc1\u0cad\u0ccb\u0ca6\u0caf!\"\n  ],\n  \"src_lang\": \"kan_Knda\",\n  \"tgt_lang\": \"eng_Latn\"\n}'\n</code></pre>"},{"location":"multimodal/translate_server.html#response_1","title":"Response","text":"<pre><code>{\n  \"translations\": [\n    \"Hello, how are you?\",\n    \"Good morning!\"\n  ]\n}\n</code></pre>"},{"location":"multimodal/translate_server.html#references","title":"References","text":"<ul> <li>IndicTrans2 Paper</li> <li>AI4Bharat IndicTrans2 Model</li> <li>AI4Bharat IndicTrans2 GitHub Repository</li> <li>IndicTransToolkit</li> <li>Extra - pip install git+https://github.com/VarunGumma/IndicTransToolkit.git</li> </ul>"},{"location":"multimodal/translate_server.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read the CONTRIBUTING.md file for guidelines on how to contribute to this project.</p> <p>Also you can join the discord group to collaborate</p>"},{"location":"multimodal/translate_server.html#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"multimodal/translate_server.html#faq","title":"FAQ","text":"<p>Q: How do I change the source and target languages?</p> <p>A: Modify the <code>compose.yaml</code> file to set the <code>SRC_LANG</code> and <code>TGT_LANG</code> variables as needed.</p> <p>Q: How do I download the translation models?</p> <p>A: Use the <code>huggingface-cli</code> commands provided in the Downloading Translation Models section.</p> <p>Q: How do I run the server locally?</p> <p>A: Follow the instructions in the Running with FastAPI Server section.</p>"},{"location":"multimodal/translate_server.html#license_1","title":"License","text":"<ul> <li>IndicTrans License</li> <li>IndicTrans Data License</li> </ul> <p>This README provides a comprehensive guide to setting up and running the Indic Translate Server. For more details, refer to the linked resources.</p>"},{"location":"multimodal/translate_server.html#citation","title":"Citation","text":"<pre><code>@article{gala2023indictrans,\ntitle={IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\nauthor={Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2023},\nurl={https://openreview.net/forum?id=vfT4YuzAYA},\nnote={}\n}\n</code></pre>"},{"location":"multimodal/tts-server.html","title":"TTS Server","text":"<ul> <li>https://github.com/dwani-ai/tts-indic-server <pre><code>git clone https://github.com/dwani-ai/tts-indic-server\ncd tts-indic-server\ngit checkout gh-200\nexport HF_TOKEN='this-my-token'\npython -m venv  venv\nsource venv/bin/activate\npip install wheel packaging\n\npip install -r requirements.txt\n\n pip uninstall torch torchaudio torchvision\n\npip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n\npython src/gh200/main.py --host 0.0.0.0 --port 7864 --config config_two\n</code></pre></li> </ul>"},{"location":"multimodal/tts-server.html#tts-text-to-speech","title":"TTS - Text to Speech","text":""},{"location":"multimodal/tts-server.html#tts-indic-server","title":"TTS Indic Server","text":""},{"location":"multimodal/tts-server.html#overview","title":"Overview","text":"<p>Text to Speech (TTS) for Indian languages using ai4bharat/IndicF5  model.</p>"},{"location":"multimodal/tts-server.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Live Server</li> <li>Usage</li> <li>How to Use the Service<ul> <li>High Latency Service</li> <li>Low Latency Service</li> </ul> </li> <li>Getting Started - Development</li> <li>For Development (Local)</li> <li>Downloading Indic TTS Model</li> <li>Running with FastAPI Server</li> <li>Evaluating Results</li> <li>Examples<ul> <li>Kannada</li> <li>Hindi</li> </ul> </li> <li>Specifying a Different Format</li> <li>Playing Back the Audio</li> <li>Describing the Voice<ul> <li>Describing the Voice with Specific Speaker - Suresh</li> <li>Describing the Voice with Specific Speaker - Anu</li> </ul> </li> <li>Building Docker Image</li> <li>Run the Docker Image</li> <li>Available Speakers</li> <li>Tips</li> <li>Description Examples</li> <li>Citations</li> </ul>"},{"location":"multimodal/tts-server.html#live-server","title":"Live Server","text":"<p>We have hosted a Text to Speech (TTS) service that can be used to verify the accuracy of Speech generation. </p> <ul> <li>Gradio Demo</li> </ul>"},{"location":"multimodal/tts-server.html#getting-started","title":"Getting Started","text":""},{"location":"multimodal/tts-server.html#for-development-local","title":"For Development (Local)","text":"<ul> <li>Prerequisites: Python 3.10, Ubuntu 22.04</li> <li>Steps:</li> <li>Create a virtual environment:   <pre><code>python -m venv venv\n</code></pre></li> <li>Activate the virtual environment:   <pre><code>source venv/bin/activate\n</code></pre>   On Windows, use:   <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></li> </ul>"},{"location":"multimodal/tts-server.html#downloading-indic-tts-model","title":"Downloading Indic TTS Model","text":"<p>```bash download_model.sh huggingface_cli download ai4bharat/IndicF5 <pre><code>### Local Model Run\n```python\nfrom transformers import AutoModel\nimport numpy as np\nimport soundfile as sf\n\n# Load INF5 from Hugging Face\nrepo_id = \"ai4bharat/IndicF5\"\nmodel = AutoModel.from_pretrained(repo_id, trust_remote_code=True)\n\n\n# Generate speech\naudio = model(\n    \"\u0cac\u0cc6\u0c82\u0c97\u0cb3\u0cc2\u0cb0\u0cc1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0cb0\u0cbe\u0c9c\u0ca7\u0cbe\u0ca8\u0cbf \u0c86\u0c97\u0cbf\u0ca6\u0cc6, \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0ca8\u0cbe\u0cb5\u0cc1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cc7\u0cb5\u0cc6\",\n    ref_audio_path=\"prompts/KAN_F_HAPPY_00001.wav\",\n    ref_text=\"\u0ca8\u0cae\u0ccd\u200c \u0cab\u0ccd\u0cb0\u0cbf\u0c9c\u0ccd\u0c9c\u0cb2\u0ccd\u0cb2\u0cbf  \u0c95\u0cc2\u0cb2\u0cbf\u0c82\u0c97\u0ccd\u200c \u0cb8\u0cae\u0cb8\u0ccd\u0caf\u0cc6 \u0c86\u0c97\u0cbf \u0ca8\u0cbe\u0ca8\u0ccd\u200c \u0cad\u0cbe\u0cb3 \u0ca6\u0cbf\u0ca8\u0ca6\u0cbf\u0c82\u0ca6 \u0c92\u0ca6\u0ccd\u0ca6\u0cbe\u0ca1\u0ccd\u0ca4\u0cbf\u0ca6\u0ccd\u0ca6\u0cc6, \u0c86\u0ca6\u0ccd\u0cb0\u0cc6 \u0c85\u0ca6\u0ccd\u0ca8\u0cc0\u0c97 \u0cae\u0cc6\u0c95\u0cbe\u0ca8\u0cbf\u0c95\u0ccd \u0c86\u0c97\u0cbf\u0cb0\u0ccb \u0ca8\u0cbf\u0cae\u0ccd\u200c \u0cb8\u0cb9\u0cbe\u0caf\u0ccd\u0ca6\u0cbf\u0c82\u0ca6 \u0cac\u0c97\u0cc6\u0cb9\u0cb0\u0cbf\u0cb8\u0ccd\u0c95\u0ccb\u0cac\u0ccb\u0ca6\u0cc1 \u0c85\u0c82\u0ca4\u0cbe\u0c97\u0cbf \u0ca8\u0cbf\u0cb0\u0cbe\u0cb3 \u0c86\u0caf\u0ccd\u0ca4\u0cc1 \u0ca8\u0c82\u0c97\u0cc6.\"\n)\n\n\n# Normalize and save output\nif audio.dtype == np.int16:\n    audio = audio.astype(np.float32) / 32768.0\nsf.write(\"namaste.wav\", np.array(audio, dtype=np.float32), samplerate=24000)\n</code></pre></p> <ul> <li>Or Run the python code <pre><code>python tts_indic_f5.py\n</code></pre></li> </ul>"},{"location":"multimodal/tts-server.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please read the CONTRIBUTING.md file for guidelines on how to contribute to this project.</p> <p>Also you can join the discord group to collaborate</p> <ul> <li>Reference<ul> <li>ai4bharat/indic-parler-tts</li> </ul> </li> </ul>"},{"location":"multimodal/tts-server.html#citations","title":"Citations","text":"<p>```bibtex citation_1.bib @misc{lacombe-etal-2024-parler-tts,   author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},   title = {Parler-TTS},   year = {2024},   publisher = {GitHub},   journal = {GitHub repository},   howpublished = {\\url{https://github.com/huggingface/parler-tts}} } <pre><code>```bibtex citation_2.bib\n@misc{lyth2024natural,\n  title = {Natural language guidance of high-fidelity text-to-speech with synthetic annotations},\n  author = {Dan Lyth and Simon King},\n  year = {2024},\n  eprint = {2402.01912},\n  archivePrefix = {arXiv},\n  primaryClass = {cs.SD}\n}\n</code></pre></p> <pre><code>@misc{AI4Bharat_IndicF5_2025,\n  author       = {Praveen S V and Srija Anand and Soma Siddhartha and Mitesh M. Khapra},\n  title        = {IndicF5: High-Quality Text-to-Speech for Indian Languages},\n  year         = {2025},\n  url          = {https://github.com/AI4Bharat/IndicF5},\n}\n</code></pre>"},{"location":"todo/index.html","title":"Index","text":"<p>Discovery Lite</p> <ul> <li>llama.cpp</li> <li>moondream</li> <li></li> </ul>"},{"location":"todo/todo_arm_64_arm64-gpu-tools.html","title":"arm64 upgrade","text":"<p>arm64 - GPU tools </p> <p>Maintain wheels for vllm and olmocr for 1 year </p> <p>Add support to Gemma, Qwen, Mistral and Deepseek models </p> <p>Keep wheel size optimized- use proper build arguments </p> <p>--</p> <p>Measure token speed/ with and without docker </p>"},{"location":"todo/todo_arm_64_arm64-gpu-tools.html#measure-olmocr-text-extraction-times","title":"Measure - olmOCR - text extraction times","text":"<p>Vision models -  Enable fast processor  ?  To get better speed for document extraction </p> <p>--  Use bits and bytes for qunatization of models</p> <p>--</p> <p>Create GitHuB action runner to build the image </p> <p>Copy complete logic for image building from jetson containers </p>"},{"location":"tools/index.html","title":"Index","text":"<p>Discovery Lite</p> <ul> <li>llama.cpp</li> <li>moondream</li> <li></li> </ul>"},{"location":"tools/function_call.html","title":"Function call","text":"<p>Function Call</p> <p>https://ai.google.dev/gemma/docs/capabilities/function-calling</p>"},{"location":"workshop/index.html","title":"readme","text":""},{"location":"workshop/index.html#workshop-steps","title":"Workshop steps","text":"<ul> <li>Live Website : https://workshop.dwani.ai</li> </ul>"},{"location":"workshop/index.html#for-development","title":"For Development","text":"<ul> <li>Prerequisites: Python 3.10</li> <li>Steps:</li> <li>Create a virtual environment:   <pre><code>python3 -m venv venv\n</code></pre></li> <li>Activate the virtual environment:   <pre><code>source venv/bin/activate\n</code></pre>   On Windows, use:   <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>Install dependencies:   <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Setup Credentials:   <pre><code>export DWANI_API_BASE_URL=&lt;dwnai-ip&gt;\nexport DWANI_API_KEY=&lt;dwani_key&gt;\n</code></pre></li> <li>Run the Program:</li> <li> <p><code>bash     python workshop_demo.py</code></p> </li> <li> <p>To Run the program</p> </li> <li> <p>DWANI_API_BASE_URL and DWANI_API_KEY environment variables has to be set</p> <ul> <li>export DWANI_API_BASE_URL=http://example.com</li> <li>export DWANI_API_KEY='your_api_key_here'</li> </ul> </li> <li> <p>Please email us to get the IP for dwani.ai - workshop inference server</p> </li> </ul>"}]}